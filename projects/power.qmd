---
title: "Modeling and Predicting Virginia’s Electricity Usage in the Era of Data Centers: A Statistical Learning Comparison with National Utility Trends"
date: 2025-12-10
categories: [Data Centers, Power Grid]
repo-url: https://github.com/jonathanwilsonami/stat-515
format:
  html:
    number-sections: true
    math:
      eq-prefix: "Model"
authors: 
  - "Jonathan Wilson"
  - "Sean Reilly"
---

## Abstract
Do this at the end...Just a summary section on what this paper is about...

## Introduction & Background

### Motivation
Over the past few decades, technological output has risen steadily. That gradual climb transformed into exponential growth with the advent of cloud computing, and now with the acceleration of the AI era that trajectory is poised to surge even faster. This rapid expansion is widely recognized. What’s far less understood, however, is the critical infrastructure that makes all of it possible. At the center of this digital revolution lies the often overlooked backbone of modern society: the power grid. It is the heartbeat of every computation, the lifeblood behind the flow of ones and zeros. Without this essential utility—something most of us rarely think about—the modern world as we know it would come to a halt. And no where is that more true than in the state of Virginia. 

### Virginia's Internet & Data Boom
Over the past several decades, Virginia has evolved from a strategically located government corridor into the epicenter of the modern internet. This transformation began in the early 1990s, when the Metropolitan Area Exchange–East (MAE-East)—one of the original four Network Access Points funded by the National Science Foundation—was established in Northern Virginia. Its proximity to federal agencies and research institutions such as DARPA, whose ARPANET project laid the foundations for the internet, positioned the region as a natural hub for digital networks. Over time, transatlantic subsea cable systems like MAREA-BRUSA, landing in Virginia Beach, further strengthened the state's role in global connectivity [@data-center-capital].

Today, that historical advantage has scaled into an unmatched concentration of data centers, making Virginia the digital heart of the world. An estimated 70% of global internet IP traffic is created in or routed through Loudoun County’s famed “Data Center Alley,” cementing Ashburn as the world’s central point for cloud and interconnection [@dawn-of-data]. Virginia now hosts the largest cluster of data centers anywhere on Earth—surpassing Texas, California, and Illinois—with rapid expansion fueled by skyrocketing demand for cloud computing, AI workloads, and global digital services [@cushman]. This boom is supported in part by Dominion Energy’s historically competitive electricity prices, which helped attract and retain power-intensive facilities. But it also places unprecedented strain on the state’s power grid: the very infrastructure that sustains the flow of computation. As AI and cloud demands continue to accelerate, Virginia’s electricity consumption is set to become one of the most consequential factors shaping both its economic future and the stability of its energy systems [@data-center-capital].

### Virginia's Power Demands & Hyperscaler's Damnd for Data
Virginia’s data center ecosystem has expanded at an unprecedented pace, driven largely by the accelerating demand for cloud computing and AI workloads. With more than 24,000 megawatts of planned data center capacity, Virginia surpasses every other market in the world—by more than a factor of three—in total expected power demand [@cushman]. This explosive growth reflects the state’s long-standing role as the heart of global internet traffic, as well as its strategic appeal to the companies building the modern digital economy. Nowhere is this more evident than in Northern Virginia, where annual data center absorption has shattered global records; the region reached 270 MW of bookings in 2018, while no other market worldwide has ever exceeded 70 MW in a single year [@dawn-of-data].

At the center of this surge are the hyperscalers^[A hyperscaler is a company that operates massive-scale data centers to provide large-scale cloud computing services, like computing, storage, and networking, that can be rapidly scaled to meet fluctuating demand.] major cloud and platform providers such as Microsoft, Google, Meta (Facebook), Apple, Oracle, IBM, Yahoo!, LinkedIn, Alibaba, Uber, and Dropbox. These companies require immense and rapidly scalable infrastructure, with hyperscale leases often ranging from 10 to 20 megawatts or more for a single deployment. Their adoption of point-of-delivery architectures and other repeatable, modular design patterns has enabled them to scale at a pace unmatched in the industry. The resulting demand for data is fueled by the relentless expansion of e-commerce, wireless networks, social media, streaming content, SaaS platforms, artificial intelligence, machine learning, gaming, virtual reality, and the Internet of Things. Collectively, these forces make Virginia not just a data center leader, but the world’s central engine for digital power consumption [@data-center-capital].

### The Fragile Power Grid

The power grid is essentially a giant, interconnected system that moves electricity from where it’s generated to where it’s needed. Power plants feed electricity into long-distance high-voltage transmission lines, which deliver power to substations where transformers reduce the voltage for safe distribution to homes, businesses, and—more recently—massive data centers. Because electricity cannot be stored easily at grid scale, supply and demand must remain in perfect balance at every moment. This requires constant coordination among utilities, transmission operators, and regional balancing authorities to maintain stability. When demand surges faster than supply, the grid becomes stressed, increasing the risk of brownouts or outages [@power-grid].

This balancing act becomes especially challenging in states like Virginia, where hyperscale data centers consume tens of megawatts each—orders of magnitude more than typical commercial users. As Virginia has become home to the world’s largest concentration of data centers, the strain on its power system has expanded accordingly. Each new facility adds continuous, around-the-clock demand that the grid must support without interruption. This is why long-term planning for new generation, transmission upgrades, and regional reliability is becoming central to Virginia’s energy future: the digital infrastructure powering cloud computing, AI, and global internet traffic can only grow if the physical grid underneath it can keep up.

To understand how these pressures play out across the country, it is helpful to situate Virginia within the broader structure of the United States energy landscape. The graphic below shows the U.S. Census Regions and Divisions, which group the nation into four major regions and nine divisions commonly used for national energy, economic, and demographic analysis.

![U.S. Census Regions and Divisions[^refnote].](/images/power-grid/us-census-regions-and-divisions.png){#fig-regions-divisions fig-align="center" width="100%"}

### What Our Research Contributes 

Taken together, Virginia’s outsized role in global internet infrastructure, the explosive growth of hyperscale data centers, and the increasing fragility of the power grid form the backdrop for the analysis that follows. As demand for AI, cloud computing, and digital services accelerates, understanding how this demand translates into regional electricity usage becomes critical—not only for Virginia but for the nation. This study seeks to examine how Virginia’s utilities compare to those across the United States, identify whether data-center-driven growth produces distinctive load patterns, and explore the emerging pressures placed on the grid as a result. By combining national utility data with Virginia-specific trends, our goal is to measure where Virginia stands today, how its electricity profile is changing, and what these shifts may imply for energy planning, reliability, and the future of the digital economy.

## Related Works 

As shown in recent studies [@texas-power-demand], data centers significantly increase load.

TODO: Discuss 3 (already selected) other studies and approaches that have been used to understand these issues. How does our reserach differ and what new contributions do we have to add? 

## Data 

### Dataset Overview

For this analysis we used the Annual Electric Power Industry Report (Form EIA-861) which is a U.S. government survey (considered the Census for Utilities) conducted by the Energy Information Administration (EIA) that collects detailed data from electric utilities on retail sales, revenue, and other key aspects of the electric power industry. It provides information on energy sources, demand-side management, net-metering, and other operational data to help monitor the industry's status and trends [@original-eia-data]. 

Originally we wanted to know about the impact of data centers in local communities in northern VA, had reached out to several city planners, they responded back but revealed that getting specific data on individual data centers would require coordination with individual commercial power supply companies and the individual data centers themselves which would be difficult or impossible to obtain given various NDAs that would need to be signed and potential limitations on what we could share. Alas, the annually released data that is generated compiled by the EIA contains the main companies we are interested in at least their bulk aggregated numbers which will be discussed later. For example, doing a little research on line revealed that the utility named Virginia Electric & Power Co whcih is found in the data is really part of Dominion Energy which is one of the biggest power utilities in Virginia. The dataset also allows for studying behavioral indicators identifying utilities that exhibit data center like pattern such as:

- High commercial load
- Very high commercial electricity sales
- Large sales per customer
- High commercial sales per commercial customer

These variables become proxies for “data-center–like” behavior. 

A data dictionary^[https://corgis-edu.github.io/corgis/csv/electricity/] of the various data elements used in this analysis can also be referenced. 

### Limitations 

The dataset does have some obvious limitations. For one, we are looking at a yearly aggregation of data which provides a very course view of the data. This limitation does now allow us to view individual data centers or even the break down of the individual contributors of what make up a utility. There might be some missing data points as well. Some private commercial companies might not have released certain data in the survey which might leave gaps in the analysis and greatly impact the value of our model's predictions. Despite these limitations, the EIA-861 data has been used in several analysis reports and can be viewed as a reliable data source for understanding macro trends seen in the United States power grid. Later on in the analysis section we will construct various methods of overcoming these limitations however crude they might be. 

### Data Cleaning, Processing & Assumptions 

We initially started our analysis on the 2017 EIA-861 curated dataset developed by the CORGIS Dataset Project [@data-dictionary]. However, when comparing the numbers for 2017 to the original data source the numbers were off. This might have been due to updates to upstream data that never reached the curated dataset. After this discovery we created a python script to regenerate the same exact dataset (same fields as the CORGIS Dataset) not only for 2017 but for 10 years worth of data. So 2015 to 2015. This script basically parsed each of the zipped files from source and extracted data from 3 different schedules: 

1. **Operational Data** - The data containing aggregate operational data for the source and disposition of energy and revenue information from each electric utility.
2. **Utility Data** - The data contain information on a utility’s North American Electric Reliability (NERC) regions of operation. The data also indicate a utility’s independent system operator (ISO) or regional transmission organization (RTO) and whether that utility is engaged in any of the following activities: generation, transmission, buying transmission, distribution, buying distribution, wholesale marketing, retail marketing, bundled service, or operating alternative-fueled vehicles.
3. **Sales to Ultimate Customers** - The data contain revenue, sales (in megawatthours), and customer count of electricity delivered to end-use customers by state, sector, and balancing authority. A state, service type, and balancing authority-level adjustment is made for non-respondents and for customer-sited respondents.

When it comes to data cleaning we had to handle the following and make some assumptions: 

- In EIA-861 spreadsheets a dot (.) could represent data that was suppressed (as mentioned before with private companies), not available, or did not meet a certain threshold value. For these values we decided to make zero but might use soem form of data imputation to fill these in. 
- We found only a handful of empty values in the state field. We dropped these rows as knowing where the utility is located was critical to the analysis.  

### Derived Featurs - Feature Engineering

We define the following engineered variables used throughout the analysis. Much of these variables are based on potentially biased opinions of what we might expect a data center like conditons to look like. 

#### Commercial Ratio
Measures how much of a utility’s total load comes from the commercial sector. High ratios suggest large commercial customers such as data centers.
$$
\text{Commercial\_Ratio}
\;=\;
\frac{\text{Commercial Sales}}{\text{Total Sales}}
$$

#### Sales per Customer
Captures average consumption intensity across all customers. We would expect data centers to produce extremely high sales relative to customer count, making this a strong signal.
$$
\text{SalesPerCustomer}
\;=\;
\frac{\text{Total Sales}}{\text{Total Customers}}
$$

#### Commercial Sales per Commercial Customer (SPC)
Measures electricity use per commercial customer. Data centers typically appear as a small number of commercial customers with very high consumption, pushing this metric upward.
$$
\text{Commercial\_SPC}
\;=\;
\frac{\text{Commercial Sales}}{\text{Commercial Customers}}
$$

#### Losses Ratio
Represents distribution and transmission losses relative to total usage. Extremely high-volume industrial or data center load can shift loss^[Transmission and distribution losses represent the electricity that is generated but never reaches customers. This energy is lost as heat in power lines and transformers as it travels across the grid.] patterns on the grid.[@power-loss] 
$$
\text{Losses\_Ratio}
\;=\;
\frac{\text{Losses}}{\text{Total Uses}}
$$

#### Log-Transformed Size Features
Log transforms stabilize variance and capture utility scale without being dominated by extreme outliers. During our EDA we discovered some issues with the data. Total customers and total sales vary by several orders of magnitude across U.S. utilities. Some are tiny rural co-ops; others (like Dominion) serve millions. The log transformation helps to mitigate these skewness issues and improves stablity in our modeling. In this study, logging these variables prevents large utilities from overwhelming the models and provides more interpretable patterns across the full national dataset.
$$
\log(\text{Total\_Customers}),
\qquad
\log(\text{Total\_Sales})
$$

---

#### Composite Intensity Score
Combines three load-intensity metrics (Commercial Ratio, SPC, SalesPerCustomer) into a single standardized indicator of "data-center–like" consumption profiles.

Each variable above is standardized to a z-score. The composite *Intensity Score* is then defined as:

$$
\text{IntensityScore}
\;=\;
z(\text{Commercial\_Ratio})
\;+\;
z(\text{SalesPerCustomer})
\;+\;
z(\text{Commercial\_SPC})
$$

---

#### High-Demand Label
Identifies the top decile of utilities exhibiting exceptionally intense commercial electricity use those most likely associated with data center load characteristics.

Utilities falling in the **top 10%** of the Intensity Score distribution are classified as high-demand:

$$
\text{HighDemand} = 1
\quad\text{if}\quad
\text{IntensityScore} \ge Q_{0.90}(\text{IntensityScore})
$$

and otherwise:

$$
\text{HighDemand} = 0
$$


### Exploratory Data Analysis 

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, results='hide'}
#| message: false
#| warning: false


library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(reshape2)

theme_set(theme_minimal())

elec_2023 <- read_csv("data/power/electricity_2023.csv")
elec_all  <- read_csv("data/power/electricity_2015_to_2024.csv")

# glimpse(elec_2023)
# glimpse(elec_all)

############################################################
## Data Cleaning & Feature Engineering
############################################################

# Utility to build features and clean data 
# - is_VA classifies obs if Utility.State == "VA"
# - Filter out certain important fields that are zero
# - Feature engineer 
featurize_utilities_raw <- function(df) {
  df %>%
    mutate(
      is_VA = if_else(Utility.State == "VA", 1L, 0L)
    ) %>%
    filter(
      Retail.Total.Customers > 0,
      Retail.Total.Sales > 0,
      Uses.Total > 0
    ) %>%
    mutate(
      Commercial_Ratio = Retail.Commercial.Sales / Retail.Total.Sales,
      Residential_Ratio = Retail.Residential.Sales / Retail.Total.Sales,
      Industrial_Ratio = Retail.Industrial.Sales / Retail.Total.Sales,
      SalesPerCustomer = Retail.Total.Sales / Retail.Total.Customers,
      Commercial_SPC = if_else(Retail.Commercial.Customers > 0,
                                    Retail.Commercial.Sales / Retail.Commercial.Customers,
                                    NA_real_),
      Residential_SPC = if_else(Retail.Residential.Customers > 0,
                                    Retail.Residential.Sales / Retail.Residential.Customers,
                                    NA_real_),
      Losses_Ratio = Uses.Losses / Uses.Total,
      log_Total_Sales = log(Retail.Total.Sales),
      log_Total_Customers = log(Retail.Total.Customers)
    )
}
 
elec_2023_fe <- featurize_utilities_raw(elec_2023)
elec_all_fe  <- featurize_utilities_raw(elec_all)
```

The number of reporting utilities varies substantially by state.  
Figure @fig-state-utilities shows the distribution of utilities across states in 2023, with **Virginia highlighted** for comparison. As you can see, Texas, California and New York are highly skewed and represent the vast majority of the number of utilities. In order to compare Virginia to the rest of the states we will need to handle this. 

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, results='hide'}
#| label: fig-state-utilities
#| fig-cap: "Number of utilities per state in 2023, with Virginia highlighted."
#| fig-width: 7
#| fig-height: 5

state_counts_2023 <- elec_2023_fe %>%
  count(Utility.State, sort = TRUE) %>%
  mutate(
    Highlight = if_else(Utility.State == "VA", "Virginia", "Other States")
  )

ggplot(state_counts_2023,
       aes(x = reorder(Utility.State, n), y = n, fill = Highlight)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c("Virginia" = "firebrick", "Other States" = "grey70")) +
  labs(
    title = "Number of Utilities per State (2023)",
    x = "State",
    y = "Number of Utilities",
    fill = ""
  )
```


We also examine the mix of utility business types (investor-owned, municipal, cooperative, etc.).  
Figure @fig-utility-types summarizes the **count of utilities by type** in 2023. For this particular study we are interested mostly in Investor Owned, Cooperative and Retail Power Marketers but we will consider all utilities. 

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, results='hide'}
#| label: fig-utility-types
#| fig-cap: "Number of utilities by type in 2023."
#| fig-width: 7
#| fig-height: 5

type_counts_2023 <- elec_2023_fe %>%
  count(Utility.Type, sort = TRUE)

ggplot(type_counts_2023,
       aes(x = reorder(Utility.Type, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Number of Utilities by Type (2023)",
    x = "Utility Type",
    y = "Count"
  )
```

Figure @fig-num-var-distributions shows the skewed nature of the raw variables.
Log-transforming the data (Figure @fig-num-var-distributions-log) improves interpretability.

```{r}
#| label: fig-num-var-distributions
#| fig-cap: "Distributions of key electricity variables in 2023 (original scale)."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| message: false
#| warning: false
#| error: false

############################################################
## Distribution Plots for Key Numeric Variables (2023)
############################################################

candidate_numeric_cols <- c(
  "Demand.Summer.Peak",
  "Demand.Winter.Peak",
  "Sources.Generation",
  "Sources.Purchased",
  "Uses.Total",
  "Retail.Residential.Sales",
  "Retail.Commercial.Sales",
  "Retail.Industrial.Sales",
  "Retail.Total.Sales",
  "Retail.Total.Customers"
)

num_vars_2023 <- elec_2023_fe %>%
  select(any_of(candidate_numeric_cols))

num_long_2023 <- num_vars_2023 %>%
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "value"
  )

ggplot(num_long_2023, aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ variable, scales = "free") +
  labs(
    title = "Distributions of Key Electricity Variables (2023, original scale)",
    x = "Value",
    y = "Count"
  )
```

```{r}
#| label: fig-num-var-distributions-log
#| fig-cap: "Distributions of key electricity variables in 2023 (log10 scale)."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| message: false
#| warning: false
#| error: false

ggplot(num_long_2023, aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ variable, scales = "free") +
  scale_x_log10() +
  labs(
    title = "Distributions of Key Electricity Variables (2023, log10 scale)",
    x = "Value (log10)",
    y = "Count"
  )
```

Sector composition across utilities varies widely (Figure @fig-sector-ratios-hist).
Virginia’s utilities show distinct distributional patterns compared with the rest of the U.S.
(Figure @fig-sector-ratios-va-vs-others).

```{r}
#| label: fig-sector-ratios-hist
#| fig-cap: "Distribution of key electricity sector ratios across all U.S. utilities in 2023."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| warning: false
#| message: false
#| error: false

############################################################
## 5. Sector Mix & Ratios (2023, including Virginia vs Others)
############################################################

ratio_long_2023 <- elec_2023_fe %>%
  select(
    Utility.State, is_VA,
    Commercial_Ratio, Residential_Ratio,
    Industrial_Ratio, Losses_Ratio
  ) %>%
  tidyr::pivot_longer(
    cols = c(Commercial_Ratio, Residential_Ratio, Industrial_Ratio, Losses_Ratio),
    names_to = "ratio_type",
    values_to = "value"
  )

ggplot(ratio_long_2023,
       aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ ratio_type, scales = "free") +
  labs(
    title = "Distribution of Sector Ratios (2023)",
    x = "Ratio",
    y = "Count"
  )
```

```{r}
#| label: fig-sector-ratios-va-vs-others
#| fig-cap: "Comparison of sector ratios between Virginia utilities and all other U.S. utilities in 2023."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| warning: false
#| message: false
#| error: false

ratio_long_2023_va <- ratio_long_2023 %>%
  mutate(
    Region = if_else(is_VA == 1L, "Virginia", "Other States")
  )

ggplot(ratio_long_2023_va,
       aes(x = value, fill = Region)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ ratio_type, scales = "free") +
  labs(
    title = "Sector Ratios: Virginia vs Other States (2023)",
    x = "Ratio",
    y = "Density",
    fill = "Region"
  )
```

Figure @fig-customer-intensity-va-vs-others shows that Virginia utilities generally have higher
per-customer intensity levels, especially for commercial customers.

```{r}
#| label: fig-customer-intensity-va-vs-others
#| fig-cap: "Comparison of per-customer electricity intensity measures for Virginia utilities versus all other U.S. utilities (2023). Values shown on a log scale."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| warning: false
#| message: false
#| error: false

spc_long_2023 <- elec_2023_fe %>%
  dplyr::select(
    is_VA,
    SalesPerCustomer,
    Commercial_SPC,
    Residential_SPC
  ) %>%
  tidyr::pivot_longer(
    cols = c(SalesPerCustomer, Commercial_SPC, Residential_SPC),
    names_to = "metric",
    values_to = "value"
  ) %>%
  dplyr::mutate(
    Region = if_else(is_VA == 1L, "Virginia", "Other States")
  )

ggplot(spc_long_2023,
       aes(x = Region, y = value)) +
  geom_boxplot(outlier.alpha = 0.4) +
  facet_wrap(~ metric, scales = "free_y") +
  scale_y_log10() +
  labs(
    title = "Per-Customer Electricity Intensities (2023, log scale)",
    x = "",
    y = "Value (log10)"
  )
```

Figure @fig-sales-vs-customers-2023 shows how Virginia utilities compare to national utilities 
in overall system size and total electricity demand.

```{r}
#| label: fig-sales-vs-customers-2023
#| fig-cap: "Relationship between total customers and total electricity sales (log–log scale) for U.S. utilities in 2023, highlighting Virginia utilities."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| warning: false
#| message: false
#| error: false

ggplot(elec_2023_fe,
       aes(x = Retail.Total.Customers,
           y = Retail.Total.Sales,
           color = factor(is_VA))) +
  geom_point(alpha = 0.6) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "Total Sales vs Total Customers (2023)",
    x = "Total Customers (log10)",
    y = "Total Sales (log10)",
    color = "Virginia (1 = VA)"
  ) +
  scale_color_manual(values = c("0" = "gray60", "1" = "red"))
```

Figure @fig-commercial-ratio-spc-2023 highlights the commercial load structure and intensity 
for Virginia relative to other states.

```{r}
#| label: fig-commercial-ratio-spc-2023
#| fig-cap: "Commercial sector structure versus commercial sales per customer (log scale) for U.S. utilities in 2023, with Virginia utilities highlighted."
#| fig-width: 9
#| fig-height: 7
#| echo: false
#| warning: false
#| message: false
#| error: false

ggplot(elec_2023_fe,
       aes(x = Commercial_Ratio,
           y = Commercial_SPC,
           color = factor(is_VA))) +
  geom_point(alpha = 0.6) +
  scale_y_log10() +
  labs(
    title = "Commercial Ratio vs Commercial Sales per Customer (2023)",
    x = "Commercial Ratio",
    y = "Commercial Sales per Commercial Customer (log10)",
    color = "Virginia (1 = VA)"
  ) +
  scale_color_manual(values = c("0" = "gray60", "1" = "red"))
```

Figure @fig-va-vs-us-salespercustomer shows how Virginia's average electricity 
intensity per customer compares to the national trend from 2015–2024.

```{r}
#| label: fig-va-vs-us-salespercustomer
#| fig-cap: "Average electricity sales per customer over time (2015–2024), comparing Virginia to all other U.S. states."
#| fig-width: 9
#| fig-height: 6
#| echo: false
#| message: false
#| warning: false
#| error: false

va_vs_us_year <- elec_all_fe %>%
  mutate(Region = if_else(Utility.State == "VA", "Virginia", "Other States")) %>%
  group_by(Year, Region) %>%
  summarise(
    total_sales          = sum(Retail.Total.Sales, na.rm = TRUE),
    total_customers      = sum(Retail.Total.Customers, na.rm = TRUE),
    avg_SalesPerCustomer = mean(SalesPerCustomer, na.rm = TRUE),
    avg_Commercial_Ratio = mean(Commercial_Ratio, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(va_vs_us_year,
       aes(x = Year, y = avg_SalesPerCustomer, color = Region)) +
  geom_line(size = 1) +
  labs(
    title = "Average Sales per Customer: Virginia vs Other States (2015–2024)",
    x = "Year",
    y = "Average Sales per Customer",
    color = "Region"
  ) +
  scale_color_manual(values = c("Virginia" = "red", "Other States" = "gray40"))
```

Figure @fig-va-vs-us-commercialratio illustrates how Virginia's commercial 
load share differs from the rest of the U.S. over time.

```{r}
#| label: fig-va-vs-us-commercialratio
#| fig-cap: "Average commercial electricity sales ratio over time (2015–2024), comparing Virginia to all other U.S. states."
#| fig-width: 9
#| fig-height: 6
#| echo: false
#| message: false
#| warning: false
#| error: false

ggplot(va_vs_us_year,
       aes(x = Year, y = avg_Commercial_Ratio, color = Region)) +
  geom_line(size = 1) +
  labs(
    title = "Average Commercial Ratio: Virginia vs Other States (2015–2024)",
    x = "Year",
    y = "Average Commercial Ratio",
    color = "Region"
  ) +
  scale_color_manual(values = c("Virginia" = "red", "Other States" = "gray40"))
```


## Research Questions 

For this analysis we wanted to be able to answer the following questions:

1. Can we classify which utilities exhibit “data-center–like” load characteristics based on commercial intensity and per-customer electricity usage?

2. How does Virginia compare nationally in the proportion and concentration of high-demand utilities?

3. Which states show strong dominance of high-load utilities when weighted by retail sales, customer base, and commercial sales volume?

### Building a classifier 

In this section we explore methods that will enable us to classify which utilities exhibit “data-center–like” load characteristics based on commercial intensity and per-customer electricity usage. In particular we look at various logistic regression models that will later enable us to not only identify utilities for the year we trained on but also generalize to other years. 

For all models we split our data into training and test sets using a 20% hold out for testing. We set a random seed of 123 which should allow us to have consistent results. We also use our featurize method to do feature engineering yielding out high demand characteristic features mentioned in the Data section.  

Figure @fig-high-demand-distribution shows the distribution of high demand utilities across the country for 2023. This is after we do feature engineering on our data set. As you can see there are a very small number of high demand utilities. 

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#| label: fig-high-demand-distribution
#| fig-cap: "Distribution of high demand utilities for (2023)"
#| fig-width: 9
#| fig-height: 6
#| echo: false
#| message: false
#| warning: false
#| error: false


library(glmnet) # ridge logistic
library(rpart) # classification tree
library(rpart.plot) # tree plot
library(randomForest) # random forest
library(class) # KNN
library(knitr)
library(kableExtra)

set.seed(123)

power_combined <- read.csv("data/power/electricity_2015_to_2024.csv")

featurize_utilities <- function(df_raw) {
  df_raw %>%
    filter(
      Retail.Total.Customers > 0,
      Retail.Total.Sales > 0,
      Retail.Commercial.Customers > 0,
      Retail.Commercial.Sales > 0,
      Uses.Total > 0
    ) %>%
    # Drop retail power marketers to avoid distorting commercial ratio
    filter(Utility.Type != "Retail Power Marketer") %>%
    mutate(
      Commercial_Ratio = Retail.Commercial.Sales / Retail.Total.Sales,
      SalesPerCustomer = Retail.Total.Sales / Retail.Total.Customers,
      Commercial_SPC = Retail.Commercial.Sales / Retail.Commercial.Customers,
      Losses_Ratio = Uses.Losses / Uses.Total,
      log_Total_Sales = log(Retail.Total.Sales),
      log_Total_Customers = log(Retail.Total.Customers),
      is_VA = if_else(Utility.State == "VA", 1L, 0L)
    )
}

train_year <- 2023

df_year <- power_combined %>%
  filter(Year == train_year) %>%
  featurize_utilities()

# Create z-scores and composite IntensityScore
df_year <- df_year %>%
  mutate(
    z_Commercial_Ratio = as.numeric(scale(Commercial_Ratio)),
    z_SalesPerCustomer = as.numeric(scale(SalesPerCustomer)),
    z_Commercial_SPC = as.numeric(scale(Commercial_SPC))
  ) %>%
  mutate(
    IntensityScore = z_Commercial_Ratio +
                     z_SalesPerCustomer +
                     z_Commercial_SPC
  )

# High-demand = top 10% of intensity
cutoff <- quantile(df_year$IntensityScore, 0.90, na.rm = TRUE)

df_year <- df_year %>%
  mutate(
    HighDemand_intensity = if_else(IntensityScore >= cutoff, 1L, 0L)
  )

df_year %>%
  count(HighDemand_intensity) %>%
  ggplot(aes(x = factor(HighDemand_intensity),
             y = n,
             fill = factor(HighDemand_intensity))) +
  geom_col() +
  geom_text(aes(label = n),
            vjust = -0.4,
            size = 5,
            fontface = "bold") +
  scale_fill_manual(values = c("#4C78A8", "#F58518"),
                    name = "High Demand",
                    labels = c("0 = Normal Demand", "1 = High Demand")) +
  labs(
    title = "Counts of High-Demand vs Normal-Demand Utilities (2023)",
    x = "High Demand Indicator",
    y = "Count"
  ) +
  theme_minimal(base_size = 14)


```

Variables explod and are highly skewed. 

Explain

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}

eda_vars <- c(
  "Commercial_Ratio",
  "SalesPerCustomer",
  "Commercial_SPC",
  "Losses_Ratio",
  "log_Total_Customers"
)

df_long <- df_year %>%
  select(all_of(eda_vars), HighDemand_intensity) %>%
  pivot_longer(cols = all_of(eda_vars),
               names_to = "variable",
               values_to = "value")

# Histograms (original scale)
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distributions of Predictors (original scale)")

```

Explain

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
# Histograms (log scale) to show skew
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ variable, scales = "free") +
  scale_x_log10() +
  theme_minimal() +
  labs(title = "Distributions of Predictors (log10 scale)")


```

Explain

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
# Boxplots by high-demand class
ggplot(df_long,
       aes(x = factor(HighDemand_intensity), y = value)) +
  geom_boxplot(outlier.alpha = 0.5) +
  facet_wrap(~ variable, scales = "free_y") +
  theme_minimal() +
  labs(
    x = "HighDemand_intensity (0 = normal, 1 = high)",
    y = "Value",
    title = "Boxplots of Predictors by High-Demand Class (2023)"
  )


```

We first look at the unpenalized logistic regression model. This model attempted to estimate how well our utility characteristics predict whether a utility falls into the high demand or high intensity distribution (HighDemand = 1). Although the model achieved high classification accuracy, the coefficient estimates were extremely large, indicating numerical instability. This occurs when predictors nearly perfectly separate HighDemand from non-HighDemand utilities. In such cases, the logistic maximum likelihood estimator pushes coefficients toward infinity, producing inflated parameter estimates and artificially tiny p-values. This makes interpreting Model @eq-rawlogitmodel difficult. 

Due to this instability, the individual coefficients cannot be interpreted in terms of effect sizes, odds ratios, or directionality. The model is useful only as a classifier but not as an explanatory model. The extreme magnitudes also reflect high multicollinearity among engineered intensity variables, differences in scale, and the presence of many dummy variables for utility type categories. These limitations motivate the use of penalized logistic regression or simpler composite scoring methods for more stable and interpretable results which we will explore later. 

See the results of this model in @tbl-confusion-rawlogitmodel. 

As shown in Model @eq-rawlogitmodel, the probability of a utility being classified as
high-demand depends on several structural and commercial intensity features.

$$
\begin{aligned}
\Pr(Y_i = 1 \mid X_i)
&= \frac{1}{1 + \exp(-\eta_i)} \\
\\
\eta_i =\;
&\beta_0
+ \beta_1\,\text{Commercial Ratio}_i \\
&+ \beta_2\,\text{Sales per Customer}_i \\
&+ \beta_3\,\text{Commercial SPC}_i \\
&+ \beta_4\,\text{Losses Ratio}_i \\
&+ \beta_5\,\log(\text{Total Customers}_i) \\
&+ \beta_6\,\text{is VA}_i \\
&+ \sum_k \gamma_k\,\text{Utility Type}_{ik}
\end{aligned}
$$ {#eq-rawlogitmodel}


```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#| label: tbl-confusion-rawlogitmodel
#| tbl-cap: "Confusion Matrix for Unpenalized Logistic Regression Model (Test Set)"
#| echo: false
#| message: false
#| warning: false
#| error: false

############################################################
## Train/Test Split (for 2023 models)
############################################################

n <- nrow(df_year)
set.seed(123)
train_idx <- sample(seq_len(n), size = 0.8 * n)

train <- df_year[train_idx, ]
test <- df_year[-train_idx, ]

train$Utility.Type <- factor(train$Utility.Type)
test$Utility.Type <- factor(test$Utility.Type)

############################################################
## Raw Logistic Regression Model
############################################################

model_raw <- glm(
  HighDemand_intensity ~ Commercial_Ratio + SalesPerCustomer +
    Commercial_SPC + Losses_Ratio + log_Total_Customers +
    is_VA + Utility.Type,
  data = train,
  family = binomial
)

# summary(model_raw)  # shows separation / huge coefficients

test$pred_prob_raw <- predict(model_raw, newdata = test, type = "response")
test$pred_class_raw <- ifelse(test$pred_prob_raw >= 0.5, 1L, 0L)

confusion_raw <- table(
  Predicted = test$pred_class_raw,
  Actual    = test$HighDemand_intensity
)

# Basic, theme-friendly table
kable(
  confusion_raw,
  caption = "Confusion Matrix",
  align = "c"
)

# Accuracy as a small table under it
accuracy_raw <- mean(test$pred_class_raw == test$HighDemand_intensity)

kable(
  data.frame(`Classification Accuracy` = round(accuracy_raw, 4)),
  align = "c"
)

```


The standardized logistic regression model @eq-stdlogitmodel replaces the raw intensity variables with their z-scores while keeping the same outcome (HighDemand_intensity) and covariates (is_VA and Utility.Type). Standardization improves numerical conditioning and makes predictors comparable in principle, but in this case the fitted model still shows extremely large coefficient estimates (on the order of and tiny p-values for all terms. This indicates that the underlying problem has not been resolved.

$$
\Pr(Y_i = 1 \mid X_i)
= \frac{1}{1 + \exp(-\eta_i)}
$$

$$
\begin{aligned}
\eta_i =\;
&\beta_0
+ \beta_1\,z\!\left(\text{Commercial Ratio}_i\right)
+ \beta_2\,z\!\left(\text{Sales per Customer}_i\right) \\
&+ \beta_3\,z\!\left(\text{Commercial SPC}_i\right)
+ \beta_4\,z\!\left(\text{Losses Ratio}_i\right)
+ \beta_5\,z\!\left(\log(\text{Total Customers}_i)\right) \\
&+ \beta_6\,\text{is VA}_i
+ \sum_k \gamma_k\,\text{Utility Type}_{ik}
\end{aligned}
$$ {#eq-stdlogitmodel}

As with the unstandardized model, this standardized specification achieves high classification accuracy on the test set (about 97.8% see @tbl-confusion-stdlogitmodel), but the individual coefficients cannot be interpreted as meaningful effect sizes or odds ratios. The model is functioning as an overfit classifier rather than a stable explanatory model. In practice, this motivates relying on penalized models, simpler composite indices, and descriptive comparisons.

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#| label: tbl-confusion-stdlogitmodel
#| tbl-cap: "Confusion Matrix for Standardized Logistic Regression Model (Test Set)"
#| echo: false
#| message: false
#| warning: false
#| error: false

############################################################
# Standardized Logistic Model
############################################################

scale_vars <- c(
  "Commercial_Ratio",
  "SalesPerCustomer",
  "Commercial_SPC",
  "Losses_Ratio",
  "log_Total_Customers"
)

df_year_std <- df_year %>%
  mutate(across(
    all_of(scale_vars),
    ~ as.numeric(scale(.x)),
    .names = "s_{.col}"
  ))

train_std <- df_year_std[train_idx, ]
test_std  <- df_year_std[-train_idx, ]

train_std$Utility.Type <- factor(train_std$Utility.Type)
test_std$Utility.Type  <- factor(test_std$Utility.Type)

model_std <- glm(
  HighDemand_intensity ~ s_Commercial_Ratio + s_SalesPerCustomer +
    s_Commercial_SPC + s_Losses_Ratio + s_log_Total_Customers +
    is_VA + Utility.Type,
  data = train_std,
  family = binomial
)

# summary(model_std)

test_std$pred_prob_std <- predict(model_std, newdata = test_std, type = "response")
test_std$pred_class_std <- ifelse(test_std$pred_prob_std >= 0.5, 1L, 0L)

confusion_std <- table(
  Predicted = test_std$pred_class_std,
  Actual    = test_std$HighDemand_intensity
)

kable(
  confusion_std,
  caption = "Confusion Matrix (Standardized Logistic Model)",
  align = "c"
)

# Classification accuracy
accuracy_std <- mean(test_std$pred_class_std == test_std$HighDemand_intensity)

kable(
  data.frame(`Classification Accuracy` = round(accuracy_std, 4)),
  align = "c"
)

```

To address the instability observed in the unpenalized logistic regression models, we fit a Ridge-penalized logistic regression using cross-validation. Ridge regression introduces an L2 penalty on the magnitude of the coefficients, shrinking them toward zero and preventing the divergence that occurred in the raw and standardized models. We trained the model using 10-fold cross-validation, which automatically selects the value of the penalty parameter $\lambda$ that minimizes cross-validated deviance.

The Ridge model produces well-behaved coefficients and yields more reliable out-of-sample predictions. Unlike the previous models, Ridge does not aim to provide easily interpretable coefficients penalization introduces bias in exchange for substantial variance reduction but it provides a more stable and generalizable classifier. See @tbl-confusion-ridgelogitmodel. 

Classification performance on the test set remains very strong, with an accuracy near 97–98%, similar to the unpenalized models. However, because the coefficients are shrunk and closely correlated predictors share weight, Ridge should be interpreted as a predictive model, not an explanatory one.

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#| label: tbl-confusion-ridgelogitmodel
#| tbl-cap: "Confusion Matrix for Ridge Logistic Regression Model (Test Set)"
#| fig-width: 9
#| fig-height: 6
#| echo: false
#| message: false
#| warning: false
#| error: false

############################################################
# Ridge Logistic Regression
############################################################

formula_all <- HighDemand_intensity ~ Commercial_Ratio + SalesPerCustomer +
  Commercial_SPC + Losses_Ratio + log_Total_Customers +
  is_VA + Utility.Type

x_train <- model.matrix(formula_all, data = train)[, -1]
y_train <- train$HighDemand_intensity

x_test  <- model.matrix(formula_all, data = test)[, -1]
y_test  <- test$HighDemand_intensity

ridge_cv <- cv.glmnet(
  x_train,
  y_train,
  family = "binomial",
  alpha = 0
)

# plot(ridge_cv, main = "Ridge Logistic: CV Error vs Lambda") -->
# best_lambda <- ridge_cv$lambda.min -->
# best_lambda -->

pred_prob_ridge_test <- predict(ridge_cv, newx = x_test,
                                s = "lambda.min", type = "response")
pred_class_ridge_test <- ifelse(pred_prob_ridge_test >= 0.5, 1L, 0L)

# Confusion matrix
confusion_ridge <- table(
  Predicted = pred_class_ridge_test,
  Actual    = y_test
)

kable(
  confusion_ridge,
  caption = "Confusion Matrix (Ridge Logistic Model)",
  align = "c"
)

# Accuracy table
accuracy_ridge <- mean(pred_class_ridge_test == y_test)

kable(
  data.frame(`Classification Accuracy` = round(accuracy_ridge, 4)),
  align = "c"
)


```

The @tbl-logit-model-compare shows a comparison of the 3 different logistic classifier models. 

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#| label: tbl-logit-model-compare
#| tbl-cap: "Comparison of Logistic Regression Models for High-Demand Classification"
#| echo: false
#| message: false
#| warning: false

model_comparison <- data.frame(
  Model = c(
    "Raw Logistic Regression",
    "Standardized Logistic Regression",
    "Ridge Logistic Regression"
  ),
  `Training Issues` = c(
    "Severe separation; coefficients explode",
    "Still unstable; separation persists",
    "Stable; L2 penalty prevents divergence"
  ),
  `Interpretability` = c(
    "Poor (coefficients meaningless)",
    "Poor (scaled version of same issue)",
    "Moderate (penalized, but stable)"
  ),
  `AIC` = c(
    604.7,
    604.7,
    NA   # glmnet does not compute AIC
  ),
  `Test Accuracy` = c(
    round(accuracy_raw, 4),
    round(accuracy_std, 4),
    round(accuracy_ridge, 4)
  ),
  `Key Notes` = c(
    "Not reliable for inference; unstable estimates",
    "Standardization alone did not fix the problem",
    "Best predictive model; generalizes well"
  )
)

kable(model_comparison, align = "c")

```

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#| label: tbl-for
#| tbl-cap: "..."
#| echo: false
#| message: false
#| warning: false
#| error: false

# ############################################################
# ## 9. Classification Tree & Random Forest (Option C)
# ############################################################
# 
# # Tree
# tree_model <- rpart(
#   HighDemand_intensity ~ Commercial_Ratio + SalesPerCustomer +
#     Commercial_SPC + Losses_Ratio + log_Total_Customers +
#     is_VA + Utility.Type,
#   data = train,
#   method = "class"
# )
# 
# rpart.plot(tree_model, main = "Classification Tree for High-Demand Utilities")
# print(tree_model)
# 
# tree_pred <- predict(tree_model, newdata = test, type = "class")
# confusion_tree <- table(Predicted = tree_pred,
#                         Actual    = test$HighDemand_intensity)
# confusion_tree
# accuracy_tree <- mean(tree_pred == test$HighDemand_intensity)
# accuracy_tree
# 
# # Random forest
# rf_model <- randomForest(
#   factor(HighDemand_intensity) ~ Commercial_Ratio + SalesPerCustomer +
#     Commercial_SPC + Losses_Ratio + log_Total_Customers +
#     is_VA + Utility.Type,
#   data = train,
#   ntree = 500,
#   importance = TRUE
# )
# 
# print(rf_model)
# varImpPlot(rf_model, main = "Random Forest Variable Importance")
# 
# rf_pred <- predict(rf_model, newdata = test)
# confusion_rf <- table(Predicted = rf_pred,
#                       Actual    = test$HighDemand_intensity)
# confusion_rf
# accuracy_rf <- mean(rf_pred == test$HighDemand_intensity)
# accuracy_rf
# 
# ############################################################
# ## 10. KNN (on training year only, for comparison)
# ############################################################
# 
# knn_vars <- c(
#   "Commercial_Ratio",
#   "SalesPerCustomer",
#   "Commercial_SPC",
#   "Losses_Ratio",
#   "log_Total_Customers"
# )
# 
# train_means <- sapply(train[, knn_vars], mean, na.rm = TRUE)
# train_sds   <- sapply(train[, knn_vars], sd,   na.rm = TRUE)
# 
# scale_with_train <- function(df_num) {
#   as.data.frame(
#     sweep(
#       sweep(as.matrix(df_num), 2, train_means, "-"),
#       2, train_sds, "/"
#     )
#   )
# }
# 
# knn_train_X <- scale_with_train(train[, knn_vars])
# knn_test_X  <- scale_with_train(test[, knn_vars])
# 
# knn_train_y <- train$HighDemand_intensity
# knn_test_y  <- test$HighDemand_intensity
# 
# k_val <- 5
# 
# knn_pred_test <- class::knn(
#   train = knn_train_X,
#   test  = knn_test_X,
#   cl    = knn_train_y,
#   k     = k_val
# )
# 
# confusion_knn <- table(Predicted = knn_pred_test,
#                        Actual    = knn_test_y)
# confusion_knn
# accuracy_knn <- mean(knn_pred_test == knn_test_y)
# accuracy_knn
# 
# ############################################################
# ## 11. PCA + K-means (Unsupervised structure)
# ############################################################
# 
# pca_vars <- c(
#   "Commercial_Ratio",
#   "SalesPerCustomer",
#   "Commercial_SPC",
#   "Losses_Ratio",
#   "log_Total_Customers"
# )
# 
# X_pca <- df_year %>%
#   select(all_of(pca_vars)) %>%
#   scale() %>%
#   as.matrix()
# 
# pca_res <- prcomp(X_pca, center = FALSE, scale. = FALSE)
# 
# pca_var <- pca_res$sdev^2
# pca_var <- pca_var / sum(pca_var)
# 
# scree_df <- data.frame(
#   PC = seq_along(pca_var),
#   VarExplained = pca_var
# )
# 
# ggplot(scree_df, aes(x = PC, y = VarExplained)) +
#   geom_point() +
#   geom_line() +
#   scale_x_continuous(breaks = scree_df$PC) +
#   theme_minimal() +
#   labs(title = "PCA Scree Plot", y = "Proportion of Variance Explained")
# 
# df_pca <- cbind(df_year, as.data.frame(pca_res$x))
# 
# ggplot(df_pca,
#        aes(x = PC1, y = PC2,
#            color = factor(HighDemand_intensity),
#            shape = factor(is_VA))) +
#   geom_point(alpha = 0.7) +
#   theme_minimal() +
#   labs(
#     color = "HighDemand",
#     shape = "Virginia (1 = VA)",
#     title = "PCA: PC1 vs PC2 (2023)"
#   )
# 
# # K-means on first 3 PCs
# X_cluster <- df_pca %>%
#   select(PC1, PC2, PC3) %>%
#   as.matrix()
# 
# set.seed(123)
# k <- 3
# km_res <- kmeans(X_cluster, centers = k, nstart = 25)
# 
# df_pca$cluster_km <- factor(km_res$cluster)
# 
# table(df_pca$cluster_km, df_pca$HighDemand_intensity)
# table(df_pca$cluster_km, df_pca$is_VA)
# 
# ggplot(df_pca,
#        aes(x = PC1, y = PC2,
#            color = cluster_km,
#            shape = factor(is_VA))) +
#   geom_point(alpha = 0.7) +
#   theme_minimal() +
#   labs(
#     color = "K-means Cluster",
#     shape = "Virginia (1 = VA)",
#     title = "K-means Clusters in PCA Space (2023)"
#   )
# 
# ############################################################
# ## 12. Model Comparison Summary (2023)
# ############################################################
# 
# model_compare <- data.frame(
#   Model = c(
#     "Logistic (raw)",
#     "Logistic (scaled)",
#     "Ridge Logistic",
#     "Classification Tree",
#     "Random Forest",
#     "KNN (k=5)"
#   ),
#   Accuracy = c(
#     accuracy_raw,
#     accuracy_std,
#     accuracy_ridge,
#     accuracy_tree,
#     accuracy_rf,
#     accuracy_knn
#   )
# )
# 
# model_compare
# 
# ############################################################
# ## 13. Apply Trained Ridge + RF Models to ANY YEAR
# ############################################################
# 
# featurize_year <- function(df_year) {
#   featurize_utilities(df_year)
# }
# 
# classify_year_with_trained_models <- function(target_year) {
#   
#   df_year_new <- power_combined %>%
#     filter(Year == target_year) %>%
#     featurize_year()
#   
#   if (nrow(df_year_new) == 0) {
#     stop(paste("No valid rows for year", target_year))
#   }
#   
#   # Align Utility.Type with training levels
#   df_year_new$Utility.Type <- factor(df_year_new$Utility.Type,
#                                      levels = levels(train$Utility.Type))
#   
#   # Build design matrix for ridge to match x_train
#   rhs_formula <- as.formula(
#     "~ Commercial_Ratio + SalesPerCustomer +
#        Commercial_SPC + Losses_Ratio + log_Total_Customers +
#        is_VA + Utility.Type"
#   )
#   
#   x_year <- model.matrix(rhs_formula, data = df_year_new)[, -1, drop = FALSE]
#   
#   train_cols <- colnames(x_train)
#   year_cols  <- colnames(x_year)
#   
#   # Add missing columns
#   missing_cols <- setdiff(train_cols, year_cols)
#   if (length(missing_cols) > 0) {
#     for (mc in missing_cols) {
#       x_year <- cbind(x_year, 0)
#       colnames(x_year)[ncol(x_year)] <- mc
#     }
#   }
#   
#   # Drop extra columns
#   extra_cols <- setdiff(colnames(x_year), train_cols)
#   if (length(extra_cols) > 0) {
#     x_year <- x_year[, !(colnames(x_year) %in% extra_cols), drop = FALSE]
#   }
#   
#   # Reorder
#   x_year <- x_year[, train_cols, drop = FALSE]
#   
#   # Ridge predictions
#   df_year_new$pred_ridge_prob  <- as.numeric(
#     predict(ridge_cv, newx = x_year, s = "lambda.min", type = "response")
#   )
#   df_year_new$pred_ridge_class <- ifelse(df_year_new$pred_ridge_prob >= 0.5, 1L, 0L)
#   
#   # Random forest predictions
#   rf_pred <- predict(rf_model, newdata = df_year_new)
#   df_year_new$pred_rf_class <- as.integer(as.character(rf_pred))
#   
#   df_year_new
# }
# 
# ############################################################
# ## 14. Example: Classify 2023 for State Comparisons
# ############################################################
# 
# results_2023 <- classify_year_with_trained_models(2023)
# 
# # State-level summary (count-based)
# state_summary_2023 <- results_2023 %>%
#   group_by(Utility.State) %>%
#   summarise(
#     n_util          = n(),
#     n_high_ridge    = sum(pred_ridge_class == 1, na.rm = TRUE),
#     prop_high_ridge = n_high_ridge / n_util,
#     n_high_rf       = sum(pred_rf_class == 1, na.rm = TRUE),
#     prop_high_rf    = n_high_rf / n_util
#   ) %>%
#   filter(n_util >= 5)
# 
# state_summary_2023 %>%
#   arrange(desc(prop_high_ridge)) %>%
#   head(10)
# 
# ############################################################
# ## 15. Plots for Report: State Comparisons (Count-Based)
# ############################################################
# 
# # Plot 1: Bar chart of proportion high-demand by state (ridge)
# ggplot(state_summary_2023,
#        aes(x = reorder(Utility.State, prop_high_ridge),
#            y = prop_high_ridge,
#            fill = Utility.State == "VA")) +
#   geom_col() +
#   coord_flip() +
#   scale_fill_manual(values = c("grey70", "steelblue"),
#                     guide = "none") +
#   theme_minimal() +
#   labs(
#     title   = "Share of Utilities Classified as High-Demand (Ridge Model, 2023)",
#     x       = "State",
#     y       = "Proportion High-Demand Utilities",
#     caption = "High-demand inferred from 2023 structural commercial-load model"
#   )
# 
# # Plot 2: Virginia vs other states in load space (ridge predictions)
# results_2023 <- results_2023 %>%
#   mutate(
#     Region = if_else(Utility.State == "VA", "Virginia", "Other States")
#   )
# 
# ggplot(results_2023,
#        aes(x = SalesPerCustomer,
#            y = Commercial_SPC,
#            color = factor(pred_ridge_class))) +
#   geom_point(alpha = 0.6) +
#   scale_x_log10() +
#   scale_y_log10() +
#   facet_wrap(~ Region) +
#   theme_minimal() +
#   labs(
#     title  = "Utility Load Structure: Virginia vs Other States (2023)",
#     x      = "Sales per Customer (log scale)",
#     y      = "Commercial Sales per Commercial Customer (log scale)",
#     color  = "Predicted High-Demand\n(Ridge Model)"
#   )
# 
# # Plot 3: Heatmap of high-demand share by state (RF)
# ggplot(state_summary_2023,
#        aes(x = Utility.State, y = 1, fill = prop_high_rf)) +
#   geom_tile() +
#   coord_flip() +
#   scale_fill_gradient(low = "white", high = "darkred") +
#   theme_minimal() +
#   labs(
#     title = "Proportion of High-Demand Utilities by State (RF Model, 2023)",
#     x     = "State",
#     y     = NULL,
#     fill  = "Prop High-Demand"
#   ) +
#   theme(
#     axis.text.y  = element_text(size = 6),
#     axis.text.x  = element_blank(),
#     axis.ticks.x = element_blank()
#   )
# 
# ############################################################
# ## 16. Load-Weighted, Customer-Weighted, Commercial-Weighted Indices
# ##     + Composite Data Center Load Index (DCLI)
# ############################################################
# 
# # Helper to normalize to [0,1]
# normalize_01 <- function(x) {
#   rng <- range(x, na.rm = TRUE)
#   if (diff(rng) == 0) {
#     return(rep(0.5, length(x)))
#   } else {
#     (x - rng[1]) / (rng[2] - rng[1])
#   }
# }
# 
# state_indices_2023 <- results_2023 %>%
#   group_by(Utility.State) %>%
#   summarise(
#     n_util             = n(),
#     total_retail_sales = sum(Retail.Total.Sales, na.rm = TRUE),
#     total_customers    = sum(Retail.Total.Customers, na.rm = TRUE),
#     total_comm_sales   = sum(Retail.Commercial.Sales, na.rm = TRUE),
# 
#     high_sales_ridge = sum(ifelse(pred_ridge_class == 1,
#                                   Retail.Total.Sales, 0), na.rm = TRUE),
#     high_cust_ridge  = sum(ifelse(pred_ridge_class == 1,
#                                   Retail.Total.Customers, 0), na.rm = TRUE),
#     high_comm_sales_ridge = sum(ifelse(pred_ridge_class == 1,
#                                        Retail.Commercial.Sales, 0), na.rm = TRUE),
# 
#     high_sales_rf = sum(ifelse(pred_rf_class == 1,
#                                Retail.Total.Sales, 0), na.rm = TRUE),
#     high_comm_sales_rf = sum(ifelse(pred_rf_class == 1,
#                                     Retail.Commercial.Sales, 0), na.rm = TRUE)
#   ) %>%
#   filter(total_retail_sales > 0,
#          total_customers > 0) %>%
#   mutate(
#     # Load-weighted index (what you already had)
#     prop_sales_high_ridge = high_sales_ridge / total_retail_sales,
#     prop_sales_high_rf    = high_sales_rf   / total_retail_sales,
# 
#     # NEW: customer-weighted index
#     prop_customers_high_ridge = high_cust_ridge / total_customers,
# 
#     # NEW: commercial-sales-weighted index
#     prop_comm_high_ridge = if_else(
#       total_comm_sales > 0,
#       high_comm_sales_ridge / total_comm_sales,
#       NA_real_
#     ),
# 
#     # Normalized versions on [0,1] for composite index
#     idx_load_n = normalize_01(prop_sales_high_ridge),
#     idx_cust_n = normalize_01(prop_customers_high_ridge),
#     idx_comm_n = normalize_01(prop_comm_high_ridge),
# 
#     # Composite Data Center Load Index (DCLI)
#     DCLI = rowMeans(
#       cbind(idx_load_n, idx_cust_n, idx_comm_n),
#       na.rm = TRUE
#     )
#   )
# 
# # Inspect top 10 states by DCLI
# state_indices_2023 %>%
#   arrange(desc(DCLI)) %>%
#   head(10)
# 
# ############################################################
# ## 17. Calibrate Dominance Thresholds from DCLI
# ############################################################
# 
# dcli_quants <- quantile(state_indices_2023$DCLI,
#                         probs = c(0.50, 0.75, 0.90),
#                         na.rm = TRUE)
# 
# dcli_quants
# 
# state_indices_2023 <- state_indices_2023 %>%
#   mutate(
#     dominance_class = case_when(
#       DCLI >= dcli_quants[3] ~ "Extreme",
#       DCLI >= dcli_quants[2] ~ "High",
#       DCLI >= dcli_quants[1] ~ "Moderate",
#       TRUE                   ~ "Low"
#     ),
#     dominance_class = factor(
#       dominance_class,
#       levels = c("Low", "Moderate", "High", "Extreme")
#     )
#   )
# 
# # See where VA lands
# state_indices_2023 %>%
#   filter(Utility.State == "VA")
# 
# ############################################################
# ## 18. Plots: Load-Weighted Index and DCLI
# ############################################################
# 
# # Plot 4: Load-weighted high-demand index (ridge), VA highlighted
# ggplot(state_indices_2023,
#        aes(x = reorder(Utility.State, prop_sales_high_ridge),
#            y = prop_sales_high_ridge,
#            fill = Utility.State == "VA")) +
#   geom_col() +
#   coord_flip() +
#   scale_fill_manual(values = c("grey70", "darkorange"),
#                     guide = "none") +
#   theme_minimal() +
#   labs(
#     title = "Load-Weighted High-Demand Index by State (Ridge Model, 2023)",
#     x     = "State",
#     y     = "Share of Retail Sales from High-Demand Utilities",
#     caption = "Weights each utility by total retail sales; highlights states dominated by a few massive utilities."
#   )
# 
# # Plot 5: Compare count-based vs load-weighted for selected states
# focus_states <- c("VA", "NY", "NJ", "CA", "MD", "TX", "CO")
# 
# state_compare_2023 <- state_summary_2023 %>%
#   inner_join(state_indices_2023,
#              by = c("Utility.State", "n_util")) %>%
#   filter(Utility.State %in% focus_states) %>%
#   select(Utility.State,
#          prop_high_ridge,
#          prop_sales_high_ridge) %>%
#   pivot_longer(
#     cols = c(prop_high_ridge, prop_sales_high_ridge),
#     names_to = "Metric",
#     values_to = "Value"
#   )
# 
# ggplot(state_compare_2023,
#        aes(x = Utility.State, y = Value,
#            fill = Metric)) +
#   geom_col(position = "dodge") +
#   theme_minimal() +
#   labs(
#     title = "Count-Based vs Load-Weighted High-Demand Share (Ridge, 2023)",
#     x     = "State",
#     y     = "Proportion",
#     fill  = "Metric"
#   )
# 
# # Plot 6: DCLI by state with dominance classes, VA highlighted
# ggplot(state_indices_2023,
#        aes(x = reorder(Utility.State, DCLI),
#            y = DCLI,
#            fill = dominance_class)) +
#   geom_col() +
#   coord_flip() +
#   theme_minimal() +
#   scale_fill_brewer(palette = "RdYlGn", direction = -1) +
#   labs(
#     title = "Composite Data Center Load Index (DCLI) by State (2023)",
#     x     = "State",
#     y     = "DCLI (0–1, higher = more DC-like load concentration)",
#     fill  = "Dominance Class"
#   )
# 
# # Optional: zoom in on top 10 DCLI states
# top10_dcli <- state_indices_2023 %>%
#   arrange(desc(DCLI)) %>%
#   slice(1:10)
# 
# ggplot(top10_dcli,
#        aes(x = reorder(Utility.State, DCLI),
#            y = DCLI,
#            fill = dominance_class)) +
#   geom_col() +
#   coord_flip() +
#   theme_minimal() +
#   scale_fill_brewer(palette = "RdYlGn", direction = -1) +
#   labs(
#     title = "Top 10 States by Composite Data Center Load Index (DCLI)",
#     x     = "State",
#     y     = "DCLI"
#   )



```

## Conclusion

This study demonstrates that utility level EIA 861 data despite lacking explicit data-center identifiers can successfully reveal data-center–like electricity demand patterns using structural load metrics, machine learning classification, and weighted dominance indices.

Key insights gained from this study:

## References 
