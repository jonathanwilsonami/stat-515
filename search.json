[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Projects\n  \n  \n    \n     About\n  \n\n  \n  \n\nStat 515 Projects Website\nWelcome!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Jonathan Wilson\n\n\n\nI’m an engineer at heart. I grew up working in construction, earned a degree in Applied Statistics and Computer Science from BYU, and have spent more than a decade building software across diverse domains—ranging from DoD satellites (image processing, optimization) and large-scale cybersecurity analytics (petabyte-scale ML) to network engineering, monitoring, and data platforms. Today, my focus is on designing data architectures and pipelines for modeling, simulation, and analysis of large-scale systems. I’m currently pursuing a master’s in Data Analytics Engineering at GMU.\n\n\n\n\n\n\n\nSean\n\n\n\nMy name is Sean Reilly, and I currently work at CyberData Technologies, Inc. as a Business Analyst. I am also a Swim Coach for Streamline Swim Club. I graduated Lafayette College in Spring 2024 with a BS in Math and Minor in Data Science. While at Lafayette College, I was a Division 1 swimmer for all 4 years. I am a detail-oriented problem solver with a strong foundation in data analysis, data modeling, analytical thinking, communication, and collaboration. I had the amazing opportunity to take the position as my community summer swim team’s Head Coach for 3 summers while in college, pursuing a passion of mine, giving back to my community, and developing valuable skills along the way including leadership, interpersonal skills, organizational skills, and written communication."
  },
  {
    "objectID": "about.html#jonathan-wilson",
    "href": "about.html#jonathan-wilson",
    "title": "About Us",
    "section": "",
    "text": "Jonathan Wilson\n\n\n\nI’m an engineer at heart. I grew up working in construction, earned a degree in Applied Statistics and Computer Science from BYU, and have spent more than a decade building software across diverse domains—ranging from DoD satellites (image processing, optimization) and large-scale cybersecurity analytics (petabyte-scale ML) to network engineering, monitoring, and data platforms. Today, my focus is on designing data architectures and pipelines for modeling, simulation, and analysis of large-scale systems. I’m currently pursuing a master’s in Data Analytics Engineering at GMU."
  },
  {
    "objectID": "about.html#sean-reilly",
    "href": "about.html#sean-reilly",
    "title": "About Us",
    "section": "",
    "text": "Sean\n\n\n\nMy name is Sean Reilly, and I currently work at CyberData Technologies, Inc. as a Business Analyst. I am also a Swim Coach for Streamline Swim Club. I graduated Lafayette College in Spring 2024 with a BS in Math and Minor in Data Science. While at Lafayette College, I was a Division 1 swimmer for all 4 years. I am a detail-oriented problem solver with a strong foundation in data analysis, data modeling, analytical thinking, communication, and collaboration. I had the amazing opportunity to take the position as my community summer swim team’s Head Coach for 3 summers while in college, pursuing a passion of mine, giving back to my community, and developing valuable skills along the way including leadership, interpersonal skills, organizational skills, and written communication."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "A collection of visualization projects—dashboards, interactive charts, and storytelling with data.\n\n\n\n\n\n\n\n\n\n\n\n\nCrime And Poverty Redesign Project CODE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers\n\n\n\n\n\n\n\n\n\nDec 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nPossible Relationship Between Poverty & Crime in Dillon County South Carolina\n\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html",
    "href": "projects/crime-and-poverty-CODE.html",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "",
    "text": "The following code fetches the data, sets project variables, and creates several helper functions for making charts looker clearer and cleaner. The entire project uses ggplot2, dplyr, tidyr, tidyverse, maps, mapproj, patchwork, scales, plotly, viridisLite, grid, stringr, tibble libraries. Much of the helpfer functions for the maps leverage this site1\n\n# See renv.lock \nlibrary(ggplot2)\nlibrary(dplyr)\n# library(sf)\nlibrary(maps)\n# library(stringr)\nlibrary(tidyr)\nlibrary(tidyverse) \nlibrary(mapproj)\nlibrary(patchwork)\nlibrary(scales)\nlibrary(plotly)\n\n##################################\n# Data Sources \n##################################\nstate &lt;- read.csv(\"data/crime_plus_poverty_2023.csv\")\ncounty &lt;- read.csv(\"data/crime_poverty_by_county_2023.csv\", check.names = FALSE)\n\n##################################\n# Data Cleaning \n##################################\n# Droping rows where census data was not available for 2023 source county data \nif (\"no_population\" %in% names(county)) {\n  county &lt;- county %&gt;% dplyr::filter(no_population != \"true\")\n}\n\ncounty &lt;- county %&gt;%\n  mutate(\n    poverty_rate = as.numeric(poverty_rate),\n    property_crime_per_100k = as.numeric(property_crime_per_100k),\n    violent_crime_per_100k = as.numeric(violent_crime_per_100k)\n  )\n\n##################################\n# Useful functions & Vars\n##################################\ncounty_metric_cols &lt;- c(\"poverty_rate\",\n            \"property_crime_per_100k\",\n            \"violent_crime_per_100k\")\n\n# Create geometries for mapping \nus_counties &lt;- ggplot2::map_data(\"county\")  # long/lat + region (state), subregion (county)\ncounty_key  &lt;- maps::county.fips %&gt;% mutate(polyname = str_to_lower(polyname))\n\ncounties_poly &lt;- us_counties %&gt;%\n  mutate(polyname = paste(region, subregion, sep = \",\")) %&gt;%\n  left_join(county_key, by = \"polyname\") %&gt;%\n  rename(fips = fips) %&gt;%\n  filter(!is.na(fips))\n\n# Function for creating maps\nbuild_county_map &lt;- function(data) {\n  ggplot(data, aes(long, lat, group = group)) +\n    geom_polygon(color = \"gray85\", linewidth = 0.1, na.rm = TRUE) +\n    coord_quickmap() +\n    theme_void(base_size = 12) +\n    theme(\n      legend.position = \"bottom\",\n      plot.title = element_text(face = \"bold\", size = 16),\n      plot.margin = margin(4, 8, 4, 8)\n    )\n}\n\n# Helper to compute sensible upper limits (95th percentile) for crime and poverty rates \np95 &lt;- function(x) {\n  x &lt;- x[is.finite(x)]\n  if (!length(x)) return(NA_real_)\n  as.numeric(quantile(x, 0.95, na.rm = TRUE))\n}\n\n# Derive county FIPS given a data frame\n# Uses: fips OR state+county codes OR state_abbr+county name\nderive_fips &lt;- function(df, county_key = NULL, keep_unmatched = FALSE) {\n  stopifnot(is.data.frame(df))\n\n  finish &lt;- function(out) invisible(if (keep_unmatched) out else dplyr::filter(out, !is.na(.data$fips)))\n\n  # county crosswalk (polyname = \"state,county\" lowercase)\n  if (is.null(county_key)) {\n    county_key &lt;- maps::county.fips |&gt;\n      dplyr::mutate(polyname = stringr::str_to_lower(polyname))\n  }\n\n  # Already has FIPS\n  if (\"fips\" %in% names(df)) {\n    out &lt;- dplyr::mutate(df, fips = stringr::str_pad(as.character(.data$fips), 5, \"0\"))\n    return(finish(out))\n  }\n\n  # state + county numeric/string codes\n  if (all(c(\"state\", \"county\") %in% names(df))) {\n    out &lt;- dplyr::mutate(\n      df,\n      fips = paste0(\n        stringr::str_pad(as.character(.data$state),  2, \"0\"),\n        stringr::str_pad(as.character(.data$county), 3, \"0\")\n      )\n    )\n    return(finish(out))\n  }\n\n  # state_abbr + county name (match via polyname)\n  county_col &lt;- c(\"county_name\", \"county\")[c(\"county_name\", \"county\") %in% names(df)][1]\n  if (!is.na(county_col) && \"state_abbr\" %in% names(df)) {\n    states_lookup &lt;- tibble::tibble(\n      state_abbr = c(state.abb, \"DC\"),\n      state_name = c(state.name, \"District of Columbia\")\n    ) |&gt;\n      dplyr::mutate(state_name_lower = stringr::str_to_lower(state_name))\n\n    norm_county &lt;- function(x) {\n      x |&gt;\n        stringr::str_to_lower() |&gt;\n        stringr::str_replace_all(\"\\\\s+(county|parish|borough|census area|municipality|city)$\", \"\") |&gt; # Regex to extract county name\n        stringr::str_replace_all(\"^st\\\\.?\\\\s\", \"saint \")\n    }\n\n    out &lt;- df |&gt;\n      dplyr::left_join(states_lookup, by = \"state_abbr\") |&gt;\n      dplyr::mutate(polyname = paste(.data$state_name_lower, norm_county(.data[[county_col]]), sep = \",\")) |&gt;\n      dplyr::left_join(county_key, by = \"polyname\")  # adds fips\n    return(finish(out))\n  }\n\n  stop(\"derive_fips(): need one of: 'fips'; 'state'+'county'; or 'state_abbr'+county name.\")\n}\n\n# Used to make the color scale look better \nscale_fill_binned_left &lt;- function(\n  x, # numeric vector used to size bins \n  step, # bin width \n  min_cap = NULL,\n  max_cap = NULL,\n  label_fmt = label_percent(accuracy = 1),\n  name = NULL, # legend title\n  option = \"mako\", begin = 0.15, end = 0.90, direction = -1, # colormap or palette\n  bar_width_mm = 200, bar_height_mm = 12,\n  title_size = 13, label_size = 12,\n  show_separators = TRUE  # draw white separators at bin edges\n) {\n  stopifnot(is.numeric(x), is.numeric(step), step &gt; 0)\n\n  # compute \"nice\" upper bound from p95, then clamp to caps if provided\n  upper_raw &lt;- ceiling(p95(x) / step) * step\n  if (!is.null(min_cap)) upper_raw &lt;- max(min_cap, upper_raw)\n  if (!is.null(max_cap)) upper_raw &lt;- min(max_cap, upper_raw)\n\n  brks &lt;- seq(0, upper_raw, by = step)\n  labs &lt;- paste0(label_fmt(head(brks, -1)), \"–\", label_fmt(tail(brks, -1)))\n  labs &lt;- c(labs, \"\")\n\n  scale_fill_viridis_b(\n    option = option, begin = begin, end = end, direction = direction,\n    limits = range(brks),\n    breaks = brks,\n    labels = labs,\n    oob = scales::squish, na.value = \"gray95\",\n    name = name %||% \"\",\n    guide = guide_colorsteps(\n      title.position = \"top\",\n      label.position = \"top\", # labels above bar\n      label.theme  = element_text(size = label_size, hjust = 0),  # left-justify\n      ticks = show_separators,\n      ticks.colour = if (show_separators) \"white\" else NULL,\n      ticks.linewidth = if (show_separators) 2 else NULL,\n      show.limits = FALSE,\n      barwidth  = grid::unit(bar_width_mm,  \"mm\"),\n      barheight = grid::unit(bar_height_mm, \"mm\"),\n      title.theme = element_text(size = title_size, face = \"bold\",\n                                 margin = margin(b = 4))\n    )\n  )\n}\n\n`%||%` &lt;- function(a, b) if (is.null(a)) b else a\n\n# Formtter for plotly maps \napply_plotly_binned_colorbar &lt;- function(p, brks, labs, palette = \"mako\",\n                                         begin = 0.15, end = 0.90, direction = -1,\n                                         title = \"Legend\", horizontal = TRUE,\n                                         thickness = 28, len = 1, show_separators = TRUE,\n                                         title_font_size = 13, tick_font_size = 12) {\n  cols &lt;- viridisLite::mako(length(brks) - 1, begin = begin, end = end, direction = direction)\n\n  # invisible heatmap solely to carry the colorbar\n  p &lt;- p %&gt;%\n    plotly::add_heatmap(\n      z = matrix(seq(min(brks), max(brks), length.out = 2), nrow = 1),\n      showscale = TRUE,\n      colors = cols,\n      hoverinfo = \"skip\",\n      opacity = 0,\n      xaxis = \"x2\", yaxis = \"y2\"\n    ) %&gt;%\n    plotly::layout(\n      xaxis2 = list(visible = FALSE, domain = c(0, 0.001), anchor = \"y2\", overlaying = \"x\"),\n      yaxis2 = list(visible = FALSE, domain = c(0, 0.001), anchor = \"x2\", overlaying = \"y\")\n    )\n\n  idx &lt;- length(p$x$data)\n  p$x$data[[idx]]$colorbar &lt;- list(\n    title    = list(text = title, side = \"top\", font = list(size = title_font_size)),\n    tickmode = \"array\",\n    tickvals = brks,\n    ticktext = c(labs, \"\"),  # empty label on rightmost edge\n    ticks    = if (show_separators) \"outside\" else \"\",\n    tickcolor = if (show_separators) \"white\" else NULL,\n    tickwidth = if (show_separators) 2 else NULL,\n    tickfont = list(size = tick_font_size),\n    len = len,\n    thickness = thickness\n  )\n  if (horizontal) {\n    p$x$data[[idx]]$colorbar$orientation &lt;- \"h\"\n    p$x$data[[idx]]$colorbar$x &lt;- 0.5\n    p$x$data[[idx]]$colorbar$xanchor &lt;- \"center\"\n    p$x$data[[idx]]$colorbar$y &lt;- -0.22\n  }\n  p\n}"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#loading-libs-and-data",
    "href": "projects/crime-and-poverty-CODE.html#loading-libs-and-data",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "",
    "text": "The following code fetches the data, sets project variables, and creates several helper functions for making charts looker clearer and cleaner. The entire project uses ggplot2, dplyr, tidyr, tidyverse, maps, mapproj, patchwork, scales, plotly, viridisLite, grid, stringr, tibble libraries. Much of the helpfer functions for the maps leverage this site1\n\n# See renv.lock \nlibrary(ggplot2)\nlibrary(dplyr)\n# library(sf)\nlibrary(maps)\n# library(stringr)\nlibrary(tidyr)\nlibrary(tidyverse) \nlibrary(mapproj)\nlibrary(patchwork)\nlibrary(scales)\nlibrary(plotly)\n\n##################################\n# Data Sources \n##################################\nstate &lt;- read.csv(\"data/crime_plus_poverty_2023.csv\")\ncounty &lt;- read.csv(\"data/crime_poverty_by_county_2023.csv\", check.names = FALSE)\n\n##################################\n# Data Cleaning \n##################################\n# Droping rows where census data was not available for 2023 source county data \nif (\"no_population\" %in% names(county)) {\n  county &lt;- county %&gt;% dplyr::filter(no_population != \"true\")\n}\n\ncounty &lt;- county %&gt;%\n  mutate(\n    poverty_rate = as.numeric(poverty_rate),\n    property_crime_per_100k = as.numeric(property_crime_per_100k),\n    violent_crime_per_100k = as.numeric(violent_crime_per_100k)\n  )\n\n##################################\n# Useful functions & Vars\n##################################\ncounty_metric_cols &lt;- c(\"poverty_rate\",\n            \"property_crime_per_100k\",\n            \"violent_crime_per_100k\")\n\n# Create geometries for mapping \nus_counties &lt;- ggplot2::map_data(\"county\")  # long/lat + region (state), subregion (county)\ncounty_key  &lt;- maps::county.fips %&gt;% mutate(polyname = str_to_lower(polyname))\n\ncounties_poly &lt;- us_counties %&gt;%\n  mutate(polyname = paste(region, subregion, sep = \",\")) %&gt;%\n  left_join(county_key, by = \"polyname\") %&gt;%\n  rename(fips = fips) %&gt;%\n  filter(!is.na(fips))\n\n# Function for creating maps\nbuild_county_map &lt;- function(data) {\n  ggplot(data, aes(long, lat, group = group)) +\n    geom_polygon(color = \"gray85\", linewidth = 0.1, na.rm = TRUE) +\n    coord_quickmap() +\n    theme_void(base_size = 12) +\n    theme(\n      legend.position = \"bottom\",\n      plot.title = element_text(face = \"bold\", size = 16),\n      plot.margin = margin(4, 8, 4, 8)\n    )\n}\n\n# Helper to compute sensible upper limits (95th percentile) for crime and poverty rates \np95 &lt;- function(x) {\n  x &lt;- x[is.finite(x)]\n  if (!length(x)) return(NA_real_)\n  as.numeric(quantile(x, 0.95, na.rm = TRUE))\n}\n\n# Derive county FIPS given a data frame\n# Uses: fips OR state+county codes OR state_abbr+county name\nderive_fips &lt;- function(df, county_key = NULL, keep_unmatched = FALSE) {\n  stopifnot(is.data.frame(df))\n\n  finish &lt;- function(out) invisible(if (keep_unmatched) out else dplyr::filter(out, !is.na(.data$fips)))\n\n  # county crosswalk (polyname = \"state,county\" lowercase)\n  if (is.null(county_key)) {\n    county_key &lt;- maps::county.fips |&gt;\n      dplyr::mutate(polyname = stringr::str_to_lower(polyname))\n  }\n\n  # Already has FIPS\n  if (\"fips\" %in% names(df)) {\n    out &lt;- dplyr::mutate(df, fips = stringr::str_pad(as.character(.data$fips), 5, \"0\"))\n    return(finish(out))\n  }\n\n  # state + county numeric/string codes\n  if (all(c(\"state\", \"county\") %in% names(df))) {\n    out &lt;- dplyr::mutate(\n      df,\n      fips = paste0(\n        stringr::str_pad(as.character(.data$state),  2, \"0\"),\n        stringr::str_pad(as.character(.data$county), 3, \"0\")\n      )\n    )\n    return(finish(out))\n  }\n\n  # state_abbr + county name (match via polyname)\n  county_col &lt;- c(\"county_name\", \"county\")[c(\"county_name\", \"county\") %in% names(df)][1]\n  if (!is.na(county_col) && \"state_abbr\" %in% names(df)) {\n    states_lookup &lt;- tibble::tibble(\n      state_abbr = c(state.abb, \"DC\"),\n      state_name = c(state.name, \"District of Columbia\")\n    ) |&gt;\n      dplyr::mutate(state_name_lower = stringr::str_to_lower(state_name))\n\n    norm_county &lt;- function(x) {\n      x |&gt;\n        stringr::str_to_lower() |&gt;\n        stringr::str_replace_all(\"\\\\s+(county|parish|borough|census area|municipality|city)$\", \"\") |&gt; # Regex to extract county name\n        stringr::str_replace_all(\"^st\\\\.?\\\\s\", \"saint \")\n    }\n\n    out &lt;- df |&gt;\n      dplyr::left_join(states_lookup, by = \"state_abbr\") |&gt;\n      dplyr::mutate(polyname = paste(.data$state_name_lower, norm_county(.data[[county_col]]), sep = \",\")) |&gt;\n      dplyr::left_join(county_key, by = \"polyname\")  # adds fips\n    return(finish(out))\n  }\n\n  stop(\"derive_fips(): need one of: 'fips'; 'state'+'county'; or 'state_abbr'+county name.\")\n}\n\n# Used to make the color scale look better \nscale_fill_binned_left &lt;- function(\n  x, # numeric vector used to size bins \n  step, # bin width \n  min_cap = NULL,\n  max_cap = NULL,\n  label_fmt = label_percent(accuracy = 1),\n  name = NULL, # legend title\n  option = \"mako\", begin = 0.15, end = 0.90, direction = -1, # colormap or palette\n  bar_width_mm = 200, bar_height_mm = 12,\n  title_size = 13, label_size = 12,\n  show_separators = TRUE  # draw white separators at bin edges\n) {\n  stopifnot(is.numeric(x), is.numeric(step), step &gt; 0)\n\n  # compute \"nice\" upper bound from p95, then clamp to caps if provided\n  upper_raw &lt;- ceiling(p95(x) / step) * step\n  if (!is.null(min_cap)) upper_raw &lt;- max(min_cap, upper_raw)\n  if (!is.null(max_cap)) upper_raw &lt;- min(max_cap, upper_raw)\n\n  brks &lt;- seq(0, upper_raw, by = step)\n  labs &lt;- paste0(label_fmt(head(brks, -1)), \"–\", label_fmt(tail(brks, -1)))\n  labs &lt;- c(labs, \"\")\n\n  scale_fill_viridis_b(\n    option = option, begin = begin, end = end, direction = direction,\n    limits = range(brks),\n    breaks = brks,\n    labels = labs,\n    oob = scales::squish, na.value = \"gray95\",\n    name = name %||% \"\",\n    guide = guide_colorsteps(\n      title.position = \"top\",\n      label.position = \"top\", # labels above bar\n      label.theme  = element_text(size = label_size, hjust = 0),  # left-justify\n      ticks = show_separators,\n      ticks.colour = if (show_separators) \"white\" else NULL,\n      ticks.linewidth = if (show_separators) 2 else NULL,\n      show.limits = FALSE,\n      barwidth  = grid::unit(bar_width_mm,  \"mm\"),\n      barheight = grid::unit(bar_height_mm, \"mm\"),\n      title.theme = element_text(size = title_size, face = \"bold\",\n                                 margin = margin(b = 4))\n    )\n  )\n}\n\n`%||%` &lt;- function(a, b) if (is.null(a)) b else a\n\n# Formtter for plotly maps \napply_plotly_binned_colorbar &lt;- function(p, brks, labs, palette = \"mako\",\n                                         begin = 0.15, end = 0.90, direction = -1,\n                                         title = \"Legend\", horizontal = TRUE,\n                                         thickness = 28, len = 1, show_separators = TRUE,\n                                         title_font_size = 13, tick_font_size = 12) {\n  cols &lt;- viridisLite::mako(length(brks) - 1, begin = begin, end = end, direction = direction)\n\n  # invisible heatmap solely to carry the colorbar\n  p &lt;- p %&gt;%\n    plotly::add_heatmap(\n      z = matrix(seq(min(brks), max(brks), length.out = 2), nrow = 1),\n      showscale = TRUE,\n      colors = cols,\n      hoverinfo = \"skip\",\n      opacity = 0,\n      xaxis = \"x2\", yaxis = \"y2\"\n    ) %&gt;%\n    plotly::layout(\n      xaxis2 = list(visible = FALSE, domain = c(0, 0.001), anchor = \"y2\", overlaying = \"x\"),\n      yaxis2 = list(visible = FALSE, domain = c(0, 0.001), anchor = \"x2\", overlaying = \"y\")\n    )\n\n  idx &lt;- length(p$x$data)\n  p$x$data[[idx]]$colorbar &lt;- list(\n    title    = list(text = title, side = \"top\", font = list(size = title_font_size)),\n    tickmode = \"array\",\n    tickvals = brks,\n    ticktext = c(labs, \"\"),  # empty label on rightmost edge\n    ticks    = if (show_separators) \"outside\" else \"\",\n    tickcolor = if (show_separators) \"white\" else NULL,\n    tickwidth = if (show_separators) 2 else NULL,\n    tickfont = list(size = tick_font_size),\n    len = len,\n    thickness = thickness\n  )\n  if (horizontal) {\n    p$x$data[[idx]]$colorbar$orientation &lt;- \"h\"\n    p$x$data[[idx]]$colorbar$x &lt;- 0.5\n    p$x$data[[idx]]$colorbar$xanchor &lt;- \"center\"\n    p$x$data[[idx]]$colorbar$y &lt;- -0.22\n  }\n  p\n}"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#original-graphic-redesign",
    "href": "projects/crime-and-poverty-CODE.html#original-graphic-redesign",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Original Graphic Redesign",
    "text": "Original Graphic Redesign\nThis code loads U.S. state-level data on crime and poverty, then creates two interactive scatterplots using ggplot2 and plotly that visualize the relationship between property crime and violent crime, with poverty rate shown as color. The first plot includes all states, while the second excludes Washington D.C. to adjust the axis scales for better comparison. (Utilized assistance from ChatGPT2 when creating all graphs that contain a legend tracking poverty percentage.)\n\n#load state data to be used for all state-level visualizations\n#####PLOT 1#####\n\n#create plot with all 3 variables (recreate original graph)\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\ndataviz1a = ggplot(state, aes(x=crime_rate_per_100k_property,y=crime_rate_per_100k_violent,\n                              text = paste0(\"State: \", state_abbr,\n                                            \"&lt;br&gt;Poverty: \", poverty_rate_pct, \"%\",\n                                            \"&lt;br&gt;Property Crime: \", crime_rate_per_100k_property,\n                                            \"&lt;br&gt;Violent Crime: \", crime_rate_per_100k_violent)))+\n  geom_point(aes(fill=poverty_rate_pct), color = \"black\", shape = 21, size = 3.5, stroke = 0.2)+\n  theme_minimal()+\n  scale_fill_gradientn(\n    name = \"Poverty Percentage\",\n    colours = c(\"white\", \"goldenrod1\", \"darkred\"),\n    values = scales::rescale(c(0, median(state$poverty_rate_pct, na.rm = TRUE), max(state$poverty_rate_pct, na.rm = TRUE))),\n    na.value = \"grey50\", \n    labels = function(x) paste0(x, \"%\"))+ #used AI to help create code to add percent symbols in legend\n  labs(title = \"2023 Property Crime vs Violent Crime by State in Relation to Poverty\",\n       x = \"Property Crime Incidents per 100k Residents\",\n       y = \"Violent Crime Incidents per 100k Residents\")\ndataviz1a\n#convert previous plot to an interactive plotly plot\n###DISPLAY THIS PLOT ON WEBSITE###\nggplotly(dataviz1a, tooltip = \"text\")\n\n\n\n\n#####PLOT 2#####\n\n#create previous plot excluding the outlier data point (DC)\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\ndataviz1b = ggplot(state, aes(x=crime_rate_per_100k_property,y=crime_rate_per_100k_violent,\n                       text = paste0(\"State: \", state_abbr,\n                                     \"&lt;br&gt;Poverty: \", poverty_rate_pct, \"%\",\n                                     \"&lt;br&gt;Property Crime: \", crime_rate_per_100k_property,\n                                     \"&lt;br&gt;Violent Crime: \", crime_rate_per_100k_violent))) +\n  geom_point(aes(fill=poverty_rate_pct), color = \"black\", shape = 21, size = 3.5, stroke = 0.2)+\n  theme_minimal()+\n  scale_fill_gradientn(\n    name = \"Poverty Percentage\",\n    colours = c(\"white\", \"goldenrod1\", \"darkred\"),\n    values = scales::rescale(c(0, median(state$poverty_rate_pct, na.rm = TRUE), max(state$poverty_rate_pct, na.rm = TRUE))),\n    na.value = \"grey50\",\n    labels = function(x) paste0(x, \"%\"))+ #used AI to help create code to add percent symbols in legend\n  xlim(800,3000)+ \n  ylim(100,800)+\n  labs(title = \"2023 Property Crime vs Violent Crime by State in Relation to Poverty\n(D.C. Excluded)\",\n       x = \"Property Crime Incidents per 100k Residents\",\n       y = \"Violent Crime Incidents per 100k Residents\")\ndataviz1b\n#convert previous plot to an interactive plotly plot\n###DISPLAY THIS PLOT ON WEBSITE###\nggplotly(dataviz1b, tooltip = \"text\")"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#alternative-data-visualization---using-bar-charts",
    "href": "projects/crime-and-poverty-CODE.html#alternative-data-visualization---using-bar-charts",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Alternative Data Visualization - Using Bar Charts",
    "text": "Alternative Data Visualization - Using Bar Charts\nThis code creates horizontal bar charts showing property and violent crime rates by U.S. state, colored by poverty percentage, first as separate plots sorted by crime rate and then displayed as a single faceted plot using long-format data to compare both crime types side-by-side.\n\n#arrange state data by property crime rate\nstate2a &lt;- state %&gt;%\n  mutate(state_abbr = factor(state_abbr, levels = state_abbr[order(crime_rate_per_100k_property)]))\n\n#plot property crime data\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nggplot(state2a, aes(x=crime_rate_per_100k_property,y=state_abbr)) +\n  geom_bar(stat=\"identity\", aes(fill=poverty_rate_pct),color='black') +\n  labs(x=\"Property Crime Incidents per 100k Residents\",\n       y=\"States\",\n       title=\"2023 Property Crime by State in Relation to Poverty\")+\n  theme_minimal()+\n  scale_fill_gradientn(\n    name = \"Poverty Percentage\",\n    colours = c(\"white\", \"goldenrod1\", \"darkred\"),\n    values = scales::rescale(c(0, median(state$poverty_rate_pct, na.rm = TRUE), max(state$poverty_rate_pct, na.rm = TRUE))),\n    na.value = \"grey50\",\n    labels = function(x) paste0(x, \"%\")) #used AI to help create code to add percent symbols in legend\n#arrange state data by violent crime rate\nstate2b &lt;- state %&gt;%\n  mutate(state_abbr = factor(state_abbr, levels = state_abbr[order(crime_rate_per_100k_violent)]))\n\n#plot violent crime data\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nggplot(state2b, aes(x=crime_rate_per_100k_violent,y=state_abbr)) +\n  geom_bar(stat=\"identity\", aes(fill=poverty_rate_pct),color='black') +\n  labs(x=\"Violent Crime Incidents per 100k Residents\",\n       y=\"States\",\n       title=\"2023 Violent Crime by State in Relation to Poverty\")+\n  theme_minimal()+\n  scale_fill_gradientn(\n    name = \"Poverty Percentage\",\n    colours = c(\"white\", \"goldenrod1\", \"darkred\"),\n    values = scales::rescale(c(0, median(state$poverty_rate_pct, na.rm = TRUE), max(state$poverty_rate_pct, na.rm = TRUE))),\n    na.value = \"grey50\",\n    labels = function(x) paste0(x, \"%\")) #used AI to help create code to add percent symbols in legend\n#convert state data to long format, so we can combine previous 2 plots into 1\nstate_long2 &lt;- state2a %&gt;%\n  pivot_longer(cols = c(crime_rate_per_100k_property, crime_rate_per_100k_violent),\n               names_to = \"crime_type\",\n               values_to = \"crime_rate\") %&gt;%\n  mutate(crime_type = recode(crime_type,\n                             crime_rate_per_100k_property = \"Property Crime\",\n                             crime_rate_per_100k_violent = \"Violent Crime\"))\n\n\n#plot property crime and violent crime side by side\n###DISPLAY THIS PLOT ON WEBSITE###\nggplot(state_long2, aes(x = crime_rate, y = state_abbr)) +\n  geom_bar(stat = \"identity\", aes(fill = poverty_rate_pct), color = \"black\") +\n  labs(x = \"Crime Incidents per 100k Residents\",\n       y = \"States\",\n       title = \"2023 Crime by State in Relation to Poverty\",\n       fill = \"Poverty Percentage\") +\n  theme_minimal() +\n  scale_fill_gradientn(\n    name = \"Poverty Percentage\",\n    colours = c(\"white\", \"goldenrod1\", \"darkred\"),\n    values = scales::rescale(c(0, median(state$poverty_rate_pct, na.rm = TRUE), max(state$poverty_rate_pct, na.rm = TRUE))),\n    na.value = \"grey50\",\n    labels = function(x) paste0(x, \"%\"))+ #used AI to help create code to add percent symbols in legend\n  theme(\n    plot.title = element_text(face = \"bold\"))+\n  facet_wrap(~ crime_type, ncol = 2, scales = \"free_x\")"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#alternative-data-visualization---using-poverty-as-dependent-variable",
    "href": "projects/crime-and-poverty-CODE.html#alternative-data-visualization---using-poverty-as-dependent-variable",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Alternative Data Visualization - Using Poverty as Dependent Variable",
    "text": "Alternative Data Visualization - Using Poverty as Dependent Variable\nThis code creates a static scatterplot using faceting, then creates and displays a converted interactive scatterplot that visualizes the relationship between poverty rate and crime (property and violent) across U.S. states using plotly subplots. (Utilized assistance from ChatGPT3 when creating “2023 Crime vs Poverty by State, Separated by Crime Type” plot using plotly.)\n\n#create/load hw theme commonly used in class\nhw &lt;- theme_gray()+ theme(\n  plot.title=element_text(hjust=0.5),\n  plot.subtitle=element_text(hjust=0.5),\n  plot.caption=element_text(hjust=-.5),\n  \n  strip.text.y = element_blank(),\n  strip.background=element_rect(fill=rgb(.9,.95,1),\n                                colour=gray(.5), linewidth =.2),\n  \n  panel.border=element_rect(fill=FALSE,colour=gray(.70)),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.minor.x = element_blank(),\n  panel.spacing.x = unit(0.10,\"cm\"),\n  panel.spacing.y = unit(0.05,\"cm\"),\n  \n  # axis.ticks.y= element_blank()\n  axis.ticks=element_blank(),\n  axis.text=element_text(colour=\"black\"),\n  axis.text.y=element_text(margin=margin(0,3,0,3)),\n  axis.text.x=element_text(margin=margin(-1,0,3,0))\n)\n\n#create plot with only property crime and poverty data variables\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nggplot(state, aes(x=crime_rate_per_100k_property,y=poverty_rate_pct))+\n  geom_point(fill=\"green\", color = \"black\", shape = 21, size = 3, stroke = 0.5)+\n  hw+\n  labs(title = \"2023 Property Crime vs Poverty by State\",\n       x = \"Property Crime Incidents per 100k Residents\",\n       y = \"Poverty Percentage\")\n#create plot with only violent crime and poverty data variables\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nggplot(state, aes(x=crime_rate_per_100k_violent,y=poverty_rate_pct))+\n  geom_point(fill=\"blue\", color = \"black\", shape = 21, size = 3, stroke = 0.5)+\n  hw+\n  labs(title = \"2023 Violent Crime vs Poverty by State\",\n       x = \"Violent Crime Incidents per 100k Residents\",\n       y = \"Poverty Percentage\")\n#convert state data to long format, so we can combine previous 2 plots into 1\nstate_long3 &lt;- state %&gt;%\n  select(state_abbr, poverty_rate_pct,\n         crime_rate_per_100k_property,\n         crime_rate_per_100k_violent) %&gt;%\n  pivot_longer(\n    cols = c(crime_rate_per_100k_property, crime_rate_per_100k_violent),\n    names_to = \"crime_type\",\n    values_to = \"crime_rate\"\n  ) %&gt;%\n  mutate(\n    crime_type = recode(crime_type,\n                        crime_rate_per_100k_property = \"Property Crime\",\n                        crime_rate_per_100k_violent = \"Violent Crime\")\n  )\n\n#plot property crime vs poverty and violent crime vs poverty side by side on same visualization\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nggplot(state_long3, aes(x = crime_rate, y = poverty_rate_pct)) +\n  geom_point(aes(fill = crime_type), color = \"black\", shape = 21, size = 3, stroke = 0.5) +\n  scale_fill_manual(values = c(\"Property Crime\" = \"green\", \"Violent Crime\" = \"blue\")) +\n  facet_wrap(~ crime_type, ncol = 2, scales = \"free_x\") +\n  labs(\n    title = \"2023 Crime vs Poverty by State, Separated by Crime Type\",\n    x = \"Crime Incidents per 100k Residents\",\n    y = \"Poverty Percentage\",\n    fill = \"Crime Type\"\n  ) +\n  hw \n##recreate previous plot using plotly below (used AI to help create desired conversion)\n\n#create the Property Crime plot using plotly\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nplotly3a &lt;- plot_ly(\n  data = state,\n  x = ~crime_rate_per_100k_property,\n  y = ~poverty_rate_pct,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(color = \"green\", line = list(color = \"black\", width = 0.5), size = 10),\n  text = ~paste0(\"State: \", state_abbr,\n                 \"&lt;br&gt;Property Crime: \", crime_rate_per_100k_property,\n                 \"&lt;br&gt;Poverty: \", poverty_rate_pct, \"%\"),\n  hoverinfo = \"text\",\n  name = \"Property Crime\"\n) %&gt;%\n  layout(\n    title = \"Property Crime vs Poverty\",\n    xaxis = list(title = \"Property Crime Incidents \n    per 100k Residents\"),\n    yaxis = list(title = \"Poverty Percentage\")\n  )\n\n#create the Violent Crime plot using plotly\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\nplotly3b &lt;- plot_ly(\n  data = state,\n  x = ~crime_rate_per_100k_violent,\n  y = ~poverty_rate_pct,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(color = \"blue\", line = list(color = \"black\", width = 0.5), size = 10),\n  text = ~paste0(\"State: \", state_abbr,\n                 \"&lt;br&gt;Violent Crime: \", crime_rate_per_100k_violent,\n                 \"&lt;br&gt;Poverty: \", poverty_rate_pct, \"%\"),\n  hoverinfo = \"text\",\n  name = \"Violent Crime\"\n) %&gt;%\n  layout(\n    title = \"Violent Crime vs Poverty\",\n    xaxis = list(title = \"Violent Crime Incidents \n    per 100k Residents\"),\n    yaxis = list(title = \"Poverty Percentage\")\n  )\n\n#combine the two previous plots into a subplot\n###DISPLAY THIS PLOT ON WEBSITE###\nsubplot(plotly3a, plotly3b, nrows = 1, shareY = TRUE, titleX = TRUE, titleY = TRUE) %&gt;%\n  layout(\n    title = \"2023 Crime vs Poverty by State, Separated by Crime Type\",\n    showlegend = FALSE\n  )"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#comparing-crime-and-poverty-by-state-on-usa-map",
    "href": "projects/crime-and-poverty-CODE.html#comparing-crime-and-poverty-by-state-on-usa-map",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Comparing Crime and Poverty by State on USA Map",
    "text": "Comparing Crime and Poverty by State on USA Map\nThis code prepares and joins U.S. state-level geographic and crime/poverty data, then creates three choropleth maps (for poverty, property crime, and violent crime) using ggplot2, and finally combines them into a single vertically-faceted visualization.\n\n#get USA map data and save as tibble\nusa_tbl &lt;- map_data(\"state\") %&gt;% as_tibble()\n#take state data and convert all state names to lower case\nstate4 &lt;- state %&gt;%\n  mutate(state_name = str_to_lower(state_name))\n#join USE map data and state data\nusa_state_data &lt;- usa_tbl %&gt;%\n  left_join(state4, by = c(\"region\"= \"state_name\"))\n\n#create USA map containing poverty data\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\ndataviz4a = ggplot(usa_state_data, aes(long, lat, group = group)) +\n  geom_map(\n    aes(map_id = region),\n    map = usa_tbl,\n    color = \"gray80\", fill = \"gray30\", size = 0.3) +\n  coord_map(\"ortho\", orientation = c(39, -98, 0)) +\n  geom_polygon(aes( fill = poverty_rate_pct), color = \"black\") +\n  scale_fill_gradientn(\n    name = \"Poverty Percentage\",\n    colours = c(\"darkseagreen2\", \"steelblue\", \"midnightblue\"),\n    values = scales::rescale(c(0, median(state$poverty_rate_pct, na.rm = TRUE), max(state$poverty_rate_pct, na.rm = TRUE))),\n    na.value = \"grey50\", \n    labels = function(x) paste0(x, \"%\")) +\n  theme_void() +\n  labs(title = \"2023 Poverty Percentage by State\",x = \"\", y = \"\", fill = \"\") +\n  theme(\n    plot.title = element_text(size = 22, face = \"bold\", color = \"darkred\", hjust=0.5),\n    legend.title = element_text(size=12, face = \"bold\"),\n    legend.margin = margin(t = 10),\n    legend.key.size = unit(1, \"cm\"),\n    legend.position = \"top\")\ndataviz4a\n#create USA map containing property crime data\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\ndataviz4b = ggplot(usa_state_data, aes(long, lat, group = group)) +\n  geom_map(\n    aes(map_id = region),\n    map = usa_tbl,\n    color = \"gray80\", fill = \"gray30\", size = 0.3) +\n  coord_map(\"ortho\", orientation = c(39, -98, 0)) +\n  geom_polygon(aes( fill = crime_rate_per_100k_property), color = \"black\") +\n  scale_fill_gradientn(\n    name = \"Property Crime Incidents per 100k Residents\",\n    colours = c(\"darkseagreen2\", \"steelblue\", \"midnightblue\"),\n    values = scales::rescale(c(0, max(state$crime_rate_per_100k_property, na.rm = TRUE))),\n    na.value = \"grey50\") +\n  theme_void() +\n  labs(title = \"2023 Property Crime Incidents per 100,000 Residents by State\",x = \"\", y = \"\", fill = \"\") +\n  theme(\n    plot.title = element_text(size = 22, face = \"bold\", color = \"darkgreen\", hjust=0.5),\n    legend.title = element_text(size=12, face = \"bold\"),\n    legend.margin = margin(t = 10),\n    legend.key.size = unit(1, \"cm\"),\n    legend.position = \"top\")\ndataviz4b\n#create USA map containing violent crime data\n#DO NOT DISPLAY THIS PLOT ON WEBSITE#\ndataviz4c = ggplot(usa_state_data, aes(long, lat, group = group)) +\n  geom_map(\n    aes(map_id = region),\n    map = usa_tbl,\n    color = \"gray80\", fill = \"gray30\", size = 0.3) +\n  coord_map(\"ortho\", orientation = c(39, -98, 0)) +\n  geom_polygon(aes( fill = crime_rate_per_100k_violent), color = \"black\") +\n  scale_fill_gradientn(\n    name = \"Violent Crime Incidents per 100k Residents\",\n    colours = c(\"darkseagreen2\", \"steelblue\", \"midnightblue\"),\n    values = scales::rescale(c(0, max(state$crime_rate_per_100k_violent, na.rm = TRUE))),\n    na.value = \"grey50\") +\n  theme_void() +\n  labs(title = \"2023 Violent Crime Incidents per 100,000 Residents by State\",x = \"\", y = \"\", fill = \"\") +\n  theme(\n    plot.title = element_text(size = 22, face = \"bold\", color = \"darkblue\", hjust=0.5),\n    legend.title = element_text(size=12, face = \"bold\"),\n    legend.margin = margin(t = 10),\n    legend.key.size = unit(1, \"cm\"),\n    legend.position = \"top\")\ndataviz4c\n\n\n#combine the previous 3 plots\n###DISPLAY THIS PLOT ON WEBSITE###\ndataviz4 &lt;- dataviz4a / dataviz4b / dataviz4c\ndataviz4"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#comparing-crime-and-poverty-by-u.s.-county",
    "href": "projects/crime-and-poverty-CODE.html#comparing-crime-and-poverty-by-u.s.-county",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Comparing Crime and Poverty by U.S. County",
    "text": "Comparing Crime and Poverty by U.S. County\nThis code joins the county metrics to U.S. county polygons via FIPS (using the hlper function above), builds three choropleths (poverty rate, property crime per 100k, violent crime per 100k) with percentile-based binning (also leveraging the helper function), and stacks them vertically with a unified title and captions.\n\ndf &lt;- county\n\ndf &lt;- derive_fips(df)\n\n# Join metrics to polygons\nmap_df &lt;- counties_poly %&gt;%\n  left_join(\n    df %&gt;% select(fips, all_of(county_metric_cols)),\n    by = \"fips\"\n  )\n\n# Limits for each metric - 95th percentile as the upper scale limit\nlim_pov  &lt;- c(0, max(0.3, min(0.6, ceiling(p95(map_df$poverty_rate) * 10) / 10))) \nlim_prop &lt;- c(0, p95(map_df$property_crime_per_100k)) \nlim_viol &lt;- c(0, p95(map_df$violent_crime_per_100k)) \n\npoverty &lt;- build_county_map(map_df) +\n  geom_polygon(aes(fill = poverty_rate)) +\n  scale_fill_binned_left(\n    x = map_df$poverty_rate,\n    step = 0.05, \n    min_cap = 0.30, max_cap = 0.60,\n    label_fmt = scales::label_percent(accuracy = 1),\n    name = \"Poverty rate (% of residents)\"\n  ) +\n  labs(title = \"Poverty Rate by County\") +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 22, face = \"bold\"),\n    plot.title.position = \"plot\",\n    legend.justification = \"left\",\n    legend.box.just = \"left\"\n  )\n\nproperty &lt;- build_county_map(map_df) +\n  geom_polygon(aes(fill = property_crime_per_100k)) +\n  scale_fill_binned_left(\n    x = map_df$property_crime_per_100k,\n    step = 500,  # Bucket width\n    label_fmt = scales::label_number(accuracy = 1, big.mark = \",\"),\n    name = \"Property crime incidents (per 100k) residents\" \n  ) +\n  labs(title = \"Property crime incidents per 100,000 residents\") +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 22, face = \"bold\"),\n    plot.title.position = \"plot\",\n    legend.justification = \"left\",\n    legend.box.just = \"left\"\n  )\n\nviolent &lt;- build_county_map(map_df) +\n  geom_polygon(aes(fill = violent_crime_per_100k)) +\n  scale_fill_binned_left(\n    x = map_df$violent_crime_per_100k,\n    step = 100,\n    label_fmt = scales::label_number(accuracy = 1, big.mark = \",\"),\n    name = \"Violent crime incidents per 100k residents\"\n  ) +\n  labs(title = \"Violent crime incidents per 100,000 residents\") +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 22, face = \"bold\"),\n    plot.title.position = \"plot\",\n    legend.justification = \"left\",\n    legend.box.just = \"left\"\n  )\n\n# Stack each plot vertically\nstack_us &lt;-\n  (poverty / property / violent) &\n  theme(\n    plot.margin = margin(6, 10, 6, 10)\n  )\n\nstack_us +\n  plot_annotation(\n    title   = \"Crime & Poverty by U.S. County\",\n    caption = \"Note: Some county data are missing from the Census data\\nSource: FBI Crime Data API and data.census.gov for 2023\",\n    theme = theme(\n      plot.title.position = \"plot\",\n      plot.title   = element_text(size = 28, face = \"bold\", hjust = 0), \n      plot.caption.position = \"plot\", \n      plot.caption = element_text(size = 16, hjust = 0, colour = \"grey30\",\n                                  lineheight = 1.15, margin = margin(t = 10)),\n      plot.margin  = margin(8, 12, 18, 12)\n    )\n  )"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#zooming-in-on-south-carolina",
    "href": "projects/crime-and-poverty-CODE.html#zooming-in-on-south-carolina",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Zooming in on South Carolina",
    "text": "Zooming in on South Carolina\nThis code filters the U.S. county map down to South Carolina, joins in your county metrics, builds three choropleths (poverty rate, property crime per 100k, violent crime per 100k) with binned legends, and stacks them vertically under a shared title/caption. Similar to what was done above expect filtering on SC.\n\ndf &lt;- county\n\ndf &lt;- derive_fips(df)\n\n# Build South Carolina map data only\nsc_poly &lt;- counties_poly %&gt;% filter(region == \"south carolina\")\n\nsc_map &lt;- sc_poly %&gt;%\n  left_join(df %&gt;% select(fips, all_of(county_metric_cols)), by = \"fips\")\n\nlim_pov  &lt;- c(0, max(0.3, min(0.6, ceiling(p95(sc_map$poverty_rate) * 10) / 10)))\nlim_prop &lt;- c(0, {x &lt;- p95(sc_map$property_crime_per_100k); if (!is.finite(x)) max(sc_map$property_crime_per_100k, na.rm = TRUE) else x})\nlim_viol &lt;- c(0, {x &lt;- p95(sc_map$violent_crime_per_100k);  if (!is.finite(x)) max(sc_map$violent_crime_per_100k,  na.rm = TRUE) else x})\n\npoverty_sc &lt;- build_county_map(sc_map) +\n  geom_polygon(aes(fill = poverty_rate)) +\n  scale_fill_binned_left(\n    x = sc_map$poverty_rate,\n    step = 0.05,\n    min_cap = 0.30, max_cap = 0.60,\n    label_fmt = scales::label_percent(accuracy = 1),\n    name = \"Poverty rate (% of residents)\"\n  ) +\n  labs(title = \"Poverty Rate by County\") +\n  theme(\n    legend.position = \"top\",\n    plot.title.position = \"plot\",\n    legend.justification = \"left\",\n    legend.box.just = \"left\",\n    plot.title = element_text(size = 22, face = \"bold\")\n  )\n\nproperty_sc &lt;- build_county_map(sc_map) +\n  geom_polygon(aes(fill = property_crime_per_100k)) +\n  scale_fill_binned_left(\n    x = sc_map$property_crime_per_100k,\n    step = 500,\n    label_fmt = scales::label_number(accuracy = 1, big.mark = \",\"),\n    name = \"Property crime incidents per 100k residents\"\n  ) +\n  labs(title = \"Property crime incidents per 100,000 residents\") +\n  theme(\n    legend.position = \"top\",\n    plot.title.position = \"plot\",\n    legend.justification = \"left\",\n    legend.box.just = \"left\",\n    plot.title = element_text(size = 22, face = \"bold\")\n  )\n\nviolent_sc &lt;- build_county_map(sc_map) +\n  geom_polygon(aes(fill = violent_crime_per_100k)) +\n  scale_fill_binned_left(\n    x = sc_map$violent_crime_per_100k,\n    step = 200,\n    label_fmt = scales::label_number(accuracy = 1, big.mark = \",\"),\n    name = \"Violent crime incidents per 100k residents\"\n  ) +\n  labs(title = \"Violent crime incidents per 100,000 residents\") +\n  theme(\n    legend.position = \"top\",\n    plot.title.position = \"plot\",\n    legend.justification = \"left\",\n    legend.box.just = \"left\",\n    plot.title = element_text(size = 22, face = \"bold\")\n  )\n\n# Stack the three maps\nstack_sc &lt;-\n  (poverty_sc / property_sc / violent_sc) &\n  theme(plot.margin = margin(6, 10, 6, 10))\n\nstack_sc +\n  plot_annotation(\n    title   = \"South Carolina by County\",\n    caption = \"Note: Some county data are missing from the Census data\\nSource: FBI Crime Data API and data.census.gov for 2023\",\n    theme = theme(\n      plot.title.position = \"plot\",\n      plot.title   = element_text(size = 28, face = \"bold\", hjust = 0),\n      plot.caption.position = \"plot\",\n      plot.caption = element_text(size = 16, hjust = 0, colour = \"grey30\",\n                                  lineheight = 1.15, margin = margin(t = 10)),\n      plot.margin  = margin(8, 12, 18, 12)\n    )\n  )"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#interactive-plot-of-south-carolina-to-look-at-individual-counties",
    "href": "projects/crime-and-poverty-CODE.html#interactive-plot-of-south-carolina-to-look-at-individual-counties",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Interactive Plot of South Carolina to Look at Individual Counties",
    "text": "Interactive Plot of South Carolina to Look at Individual Counties\n\ndf &lt;- county\n\ndf &lt;- derive_fips(df)\n\nsc &lt;- counties_poly %&gt;%\n  dplyr::filter(region == \"south carolina\") %&gt;%\n  dplyr::left_join(df %&gt;% dplyr::select(fips, poverty_rate, property_crime_per_100k, violent_crime_per_100k),\n                   by = \"fips\") %&gt;%\n  dplyr::mutate(\n    hover_text = paste0(\n      \"County: \", stringr::str_to_title(subregion), \"&lt;br&gt;\",\n      \"Poverty rate: \", scales::percent(poverty_rate, accuracy = 0.1), \"&lt;br&gt;\",\n      \"Property /100k: \", scales::comma(property_crime_per_100k, accuracy = 1), \"&lt;br&gt;\",\n      \"Violent /100k: \",  scales::comma(violent_crime_per_100k,  accuracy = 1)\n    )\n  )\n\nstep  &lt;- 0.05\nupper &lt;- min(0.60, max(0.30, ceiling(p95(sc$poverty_rate) / step) * step))\nbrks  &lt;- seq(0, upper, by = step)\nlabs  &lt;- paste0(scales::percent(head(brks, -1), accuracy = 1), \"–\",\n                scales::percent(tail(brks, -1), accuracy = 1))\n\np_sc &lt;- ggplot(sc, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = poverty_rate, text = hover_text), color = \"gray85\", linewidth = 0.1, na.rm = TRUE) +\n  coord_quickmap() +\n  scale_fill_viridis_b(\n    option = \"mako\", begin = 0.15, end = 0.90, direction = -1, # colormap or palette\n    limits = range(brks),\n    breaks = brks,\n    labels = c(labs, \"\"),\n    oob = scales::squish, na.value = \"gray95\",\n    name = NULL, guide = \"none\"\n  ) +\n  labs(title = \"South Carolina — Poverty Rate\") +\n  theme_void(base_size = 12)\n\n# --- plotly conversion + enforced title + custom colorbar ---\np &lt;- plotly::ggplotly(p_sc, tooltip = \"text\")\np &lt;- plotly::layout(p, title = list(text = \"South Carolina — Poverty Rate\", x = 0, xanchor = \"left\"))\n\np &lt;- apply_plotly_binned_colorbar(\n  p,\n  brks = brks,\n  labs = labs,\n  title = \"Poverty rate (% of residents)\",\n  horizontal = TRUE,\n  thickness = 28, len = 1, show_separators = TRUE,\n  title_font_size = 13, tick_font_size = 12\n)\n\np"
  },
  {
    "objectID": "projects/crime-and-poverty-CODE.html#footnotes",
    "href": "projects/crime-and-poverty-CODE.html#footnotes",
    "title": "Crime And Poverty Redesign Project CODE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(https://r-graph-gallery.com/choropleth-map.html)↩︎\nOpenAI. (2025, September 27). Response to a question about adding percent symbols to a ggplot2 legend in R [Large language model]. ChatGPT. https://chat.openai.com/↩︎\nOpenAI. (2025, October 1). Response to a question about converting faceted ggplot to plotly in R [Large language model]. ChatGPT. https://chat.openai.com/↩︎"
  },
  {
    "objectID": "projects/crime-and-poverty.html",
    "href": "projects/crime-and-poverty.html",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "",
    "text": "Summary: Data Visualization Redesign Project."
  },
  {
    "objectID": "projects/crime-and-poverty.html#introduction",
    "href": "projects/crime-and-poverty.html#introduction",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Introduction",
    "text": "Introduction\nThe task of this project is to find an existing data visualization, critically analyze its context and shortcomings, create multiple improved redesigns to reveal new patterns, compare the effectiveness of the original and redesigned plots, and provide thoughtful commentary on the process, challenges, and potential next steps while providing a story we can tell from the data."
  },
  {
    "objectID": "projects/crime-and-poverty.html#original-graphic",
    "href": "projects/crime-and-poverty.html#original-graphic",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Original Graphic",
    "text": "Original Graphic\n\n\n\n\n\n\nFigure 1: The original chart1.\n\n\n\nThe visualization shown above (see Figure 1) is the original graph that we chose for this redesign project. This visualization was taken from a website that identified multiple examples of data visualizations that do not follow best practices. Just at first glance, we can very quickly determine that there is a lot going on in this graph. In fact, there is too much going on to the point that it is hard for the audience to understand the intended story behind the graph.\n\nThe Good\n\nThe chart attempts to track three variables (property crime, violent crime, and poverty rate) in the same visualization which can be insightful to examine their intertwined relationships.\nUsing bubble charts is actually not a bad idea here and when used well it can actually be insightful allowing for proportions to be distinguished.\n\n\n\nThe Bad\nOn the other hand the chart did not do a great job in portraying the data in such a way that the audience can easily interpret and did poorly is several areas:\n\nThe bubble radius for the poverty variable has no numeric reference, so even though they are comparable between states, any single radius has no meaning on its own.\nAdditionally, there is no title of the graph so it is unclear what timeframe this data is coming from.\nThe state labels are messy and hard to read\nThe units could be more explicit.\nThe color choice is not the best.\n\nOverall, there are several changes that can be made to this original graph to better illustrate the data and the message."
  },
  {
    "objectID": "projects/crime-and-poverty.html#our-proposed-redesign",
    "href": "projects/crime-and-poverty.html#our-proposed-redesign",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Our Proposed Redesign",
    "text": "Our Proposed Redesign\nIn our redesigns, the primary goal is to make the visualizations easier to understand than the original graph. This entails reducing the level of effort needed to interpret the visualizations. There are some elements of the original graph that had good intentions behind them though, that are worth incorporating in our redesigns but we will also include several improvements and extensions to the original in several ways:\n\nThe concept of tracking three variables in the same visualization can be insightful to examine their intertwined relationships. However, we can implement this concept more effectively by simplifying variable visuals and reducing overall visual clutter within the graph.\nAdditionally, we can create multiple graphs side by side for comparison between these variables, with each graph focusing on only one or two variables.\nFor another example, the concept of comparing variables between states is important in telling a story about the data. However, we can implement this concept more effectively by cleaning up the state labels and even utilizing a different element, such as a color gradient, to differentiate between the multiple poverty levels.\nAdditionally, we can even move away from classic dot plots and explore plotting this data on a map, as most readers are familiar with a map of the US.\n\nThese are just some of the various solutions that we implemented in our redesigns."
  },
  {
    "objectID": "projects/crime-and-poverty.html#data-sources",
    "href": "projects/crime-and-poverty.html#data-sources",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Data Sources",
    "text": "Data Sources\nThe original chart did not cite its data source. Given the topic (crime and poverty rates), we assumed the data should come from authoritative databases. For this redesign, we performed substantial data preparation (described next) using the sources listed below:\n\nFBI Crime Data API2.\n\n\nThe United States Census Bureau3.\n\n\nA Note On Data Wrangling\nWe pulled 2023 data from two APIs—the U.S. Census Bureau (population and poverty) and the FBI Crime Data API—and used the Polars Python library to clean and reshape it into an analysis-ready dataset. The Census API let us retrieve state- and county-level values in a single request for 2023, but some counties lacked population (and therefore poverty rates), and a few areas (e.g., Louisiana) were missing from the pull.\nFor crime, we queried two FBI endpoints roughly ~50,000 times using ORIs (Originating Agency Identifiers) that uniquely identify police agencies within counties. After fetching, we joined the Census and crime records on state and county, then performed the required aggregations and calculated per-100,000 incident rates.\n\n\nKey Metrics & Derivations\nThis project uses three key metrics namely:\n\nPoverty rate\n\\[\n\\text{PovertyRate (\\%)} \\;=\\; \\frac{N_{\\text{below poverty}}}{N_{\\text{poverty universe}}}\\times 100\n\\]\n(As a proportion: ( = ).)\nProperty crime incidents per 100,000 residents\n\\[\n\\text{PropertyCrimeRate}_{/100k} \\;=\\; \\frac{N_{\\text{property incidents}}}{N_{\\text{population}}}\\times 100{,}000\n\\]\nViolent crime incidents per 100,000 residents\n\\[\n\\text{ViolentCrimeRate}_{/100k} \\;=\\; \\frac{N_{\\text{violent incidents}}}{N_{\\text{population}}}\\times 100{,}000\n\\]\n\n\\[\n\\begin{aligned}\n\\textbf{Where:}\\quad & \\\\\nN_{\\text{population}} &:= \\text{total population}\\\\\nN_{\\text{below poverty}} &:= \\text{count below poverty}\\\\\nN_{\\text{property incidents}} &:= \\text{total reported property crime incidents in the period (e.g., 2023)}\\\\\nN_{\\text{violent incidents}} &:= \\text{total reported violent crime incidents in the period (e.g., 2023)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "projects/crime-and-poverty.html#original-graphic-redesign",
    "href": "projects/crime-and-poverty.html#original-graphic-redesign",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Original Graphic Redesign",
    "text": "Original Graphic Redesign\nThe goal of our first redesign is to recreate the original graphic keeping most elements the same, but with improved labeling, less visual clutter, and clear interpretation of the poverty variable. More specifically, we will add a title that explains the nature and origin of the data, use a color gradient to differentiate between poverty levels, and transition the state labels to an interactive element of the visualization.\n\n\n\n\n\n\nNote that we can quickly see that the District of Columbia (DC) has significantly more crime incidents (both property crime and violent crime) than any US state, which stretches the scales of the axes. Let’s recreate this graph, excluding the outlier data point of DC, so that we can identify potential trends between the 50 states more clearly.\n\n\n\n\n\n\nNow let’s analyze the redesign shown above. When viewing this visualization, we can identify a positive relationship between property crime and violent crime much easier than in the original graphic. In other words, the more property crime incidents a state experiences often correlates with more violent crime incidents. Additionally, it is worth taking note that there are multiple states with higher poverty percentages scattered throughout the plot (look at Mississippi, West Virgina, Kentucky, Louisiana, and New Mexico)."
  },
  {
    "objectID": "projects/crime-and-poverty.html#alternative-data-visualization---using-bar-charts",
    "href": "projects/crime-and-poverty.html#alternative-data-visualization---using-bar-charts",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Alternative Data Visualization - Using Bar Charts",
    "text": "Alternative Data Visualization - Using Bar Charts\nThere are several alternative ways to illustrate the same information from the original graphic. The redesign below portrays the same data but in a familiar way by using two bar charts side by side, with one for property crime data by state and one for violent crime data by state. One positive of this type of visualization is that the state abbreviation labels are clearly displayed without requiring any interactive elements. Another positive is that these bar charts allow for even easier comparisons between state data, as states’ bars are arranged by their property crime data.\n\n\n\n\n\n\n\n\n\nWhen comparing the two bar charts, the previously identified positive trend between property crime and violent crime is shown, although it is not exactly linear. Upon examining the visualization more closely, it is interesting that Mississippi exhibits relatively low property crime and violent crime given its high poverty percentage. Conversely, it is interesting that Colorado has such a low poverty percentage given that it exhibits higher property crime and violent crime. This could indicate the lack of a substantial relationship between poverty percentage and crime incidents (property crime and violent crime) at the state level."
  },
  {
    "objectID": "projects/crime-and-poverty.html#alternative-data-visualization---using-poverty-as-dependent-variable",
    "href": "projects/crime-and-poverty.html#alternative-data-visualization---using-poverty-as-dependent-variable",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Alternative Data Visualization - Using Poverty as Dependent Variable",
    "text": "Alternative Data Visualization - Using Poverty as Dependent Variable\nWhile our previous graphs highlight more of the relationship between property crime and violent crime, the goal of the visualization below is to succinctly investigate the relationship between poverty and crime, by tracking poverty percentage as the dependent variable instead of a color gradient. This is done by creating two separate scatterplots side by side, one for property crime versus poverty and one for violent crime versus property.\n\n\n\n\n\n\nWhen analyzing these scatterplots of state-level data, we cannot identify a clear correlation between property crime and poverty, while only a slightly positive correlation between violent crime and poverty is observed. This confirms our suspicion that there is no substantial relationship between poverty percentage and crime incidents (property crime and violent crime) at the state level based on our data. Given this information, let’s visualize the data in one more way before determining the next steps."
  },
  {
    "objectID": "projects/crime-and-poverty.html#comparing-crime-and-poverty-by-state-on-usa-map",
    "href": "projects/crime-and-poverty.html#comparing-crime-and-poverty-by-state-on-usa-map",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Comparing Crime and Poverty by State on USA Map",
    "text": "Comparing Crime and Poverty by State on USA Map\nAfter identifying some of the relationships between property crime, violent crime, and poverty using bubble charts, bar charts, and scatterplots, let’s plot this data on a geographical map4 that everyone is familiar with. The visualization below displays three US maps, each tracking a different variable by a common color gradient, making it easier to spot states of interest and see how the metrics move together.\n\n\n\n\n\n\n\n\n\nThere are a few regions in the US that exhibit a relatively darker hue on all three maps. These regions include the southern Midwest states, states in the mid-southern West Coast area, and a few states in the mid-southern East Coast area. However, analyzing broader state data may not be the most effective route to capture more comprehensive trends. To better understand the relationship between poverty and crime in the US, the next step is to examine data at the county level, allowing for more detailed and comprehensive analysis."
  },
  {
    "objectID": "projects/crime-and-poverty.html#comparing-crime-and-poverty-by-u.s.-county",
    "href": "projects/crime-and-poverty.html#comparing-crime-and-poverty-by-u.s.-county",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Comparing Crime and Poverty by U.S. County",
    "text": "Comparing Crime and Poverty by U.S. County\nState-level data is useful for identifying broad, statewide trends, but county-level views let us zoom into specific pockets within states to understand patterns at a finer, more granular level. By comparing the three maps side by side, the color saturation makes it easier to spot counties of interest and see how the metrics move together.\nOne state that suggests a clear pattern is South Carolina. Looking across all three charts, a region stands out with consistently darker hues—yet it’s still a bit hard to tell exactly which county is driving that signal."
  },
  {
    "objectID": "projects/crime-and-poverty.html#zooming-in-on-south-carolina",
    "href": "projects/crime-and-poverty.html#zooming-in-on-south-carolina",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Zooming in on South Carolina",
    "text": "Zooming in on South Carolina\nContinuing our analysis, we remove the broader noise and focus on South Carolina using the same chart type. In the upper-right corner, one county consistently exhibits the darkest hue across all three metrics—poverty rate, property crime, and violent crime. Which county is that?"
  },
  {
    "objectID": "projects/crime-and-poverty.html#interactive-plot-of-south-carolina-to-look-at-individual-counties",
    "href": "projects/crime-and-poverty.html#interactive-plot-of-south-carolina-to-look-at-individual-counties",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Interactive Plot of South Carolina to Look at Individual Counties",
    "text": "Interactive Plot of South Carolina to Look at Individual Counties\nTo explore all counties across the metrics, use the interactive Plotly chart to identify each county. Hover to see names and values, and use the zoom/pan controls in the top-right modebar; to find Dillon County, zoom into the upper-right corner of South Carolina and hover until the tooltip shows “Dillon County.”"
  },
  {
    "objectID": "projects/crime-and-poverty.html#from-insight-to-action-dillon-countys-povertycrime-overlap",
    "href": "projects/crime-and-poverty.html#from-insight-to-action-dillon-countys-povertycrime-overlap",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "From Insight to Action: Dillon County’s Poverty–Crime Overlap",
    "text": "From Insight to Action: Dillon County’s Poverty–Crime Overlap\nThe chart below uses a callout (“bang box”) to highlight the county that appears most at risk. Our call to action notes a strong apparent relationship between poverty rates and crime in Dillon County. While this is not an advanced statistical analysis, cursory online research suggests that through 2023 both crime and poverty were very high according to several reports5, and at least one source indicates some action was taken in 20246.\nBefore this analysis, we weren’t even aware of Dillon County; data visualization helped us surface a real-world issue. We will forgo making any claims about causality for now—further research is needed, ideally leveraging additional datasets such as Data Commons7 and data on drug use, among others, to build a more comprehensive picture.\n\n\n\nFrom Insight to Action: Dillon County’s Poverty–Crime Overlap"
  },
  {
    "objectID": "projects/crime-and-poverty.html#video-demo",
    "href": "projects/crime-and-poverty.html#video-demo",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Video Demo",
    "text": "Video Demo"
  },
  {
    "objectID": "projects/crime-and-poverty.html#footnotes",
    "href": "projects/crime-and-poverty.html#footnotes",
    "title": "Possible Relationship Between Poverty & Crime in Dillon County South Carolina",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGupta, A. (2022, February 23). 10 Good and Bad Examples of Data Visualization · Polymer. Polymer. https://www.polymersearch.com/blog/10-good-and-bad-examples-of-data-visualization↩︎\nFederal Bureau of Investigation. (n.d.). FBI Crime Data API [Web service and data API]. U.S. Department of Justice. Retrieved October 5, 2025, from https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/docApi U.S. Census Bureau. (n.d.). data.census.gov [Data portal]. Retrieved October 5, 2025, from https://data.census.gov/↩︎\nU.S. Census Bureau. (n.d.). data.census.gov [Data portal]. Retrieved October 5, 2025, from https://data.census.gov/↩︎\nDassanayake (2023). Example 2: Republican voting in 1976 by state [HTML file]. In Maps (maps (4).html). Retrieved from file:///Users/sean/Downloads/maps%20(4).html#example-2-republican-voting-in-1976-by-state↩︎\nhttps://www.wbtw.com/news/pee-dee/dillon-county/dillon-county-sheriff-talks-about-decreasing-crime-after-sleds-2023-report/↩︎\nhttps://wpde.com/news/local/violent-crime-rates-down-29-percent-in-dillon-county-says-sled-south-carolina-state-law-enforcement-division-statistics-murder-sexual-battery-robbery-aggravated-assault-breaking-entering-car-theft-larceny-arson-reduction-charges-investigations↩︎\nhttps://datacommons.org/place/geoId/45033↩︎"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome! Explore featured work across Data Viz, AI/ML, IoT & Electronics, and Construction.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/power.html",
    "href": "projects/power.html",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "",
    "text": "The rapid expansion of data centers in Virginia has raised concerns about rising electricity consumption and shifting load patterns especially in northern counties of the state. This study leverages a statistical learning approach to identify high-demand electric utilities and to compare Virginia’s demand characteristics with national trends. Using ten years of U.S. Energy Information Administration (EIA-861) data, we construct engineered features such as commercial sales ratios, per-customer intensity metrics, and loss ratios—and compute a composite Intensity Score to classify utilities exhibiting unusually high load profiles.\nMultiple supervised models are evaluated, including logistic regression, standardized logistic regression, ridge-penalized logistic regression, classification trees, random forests, and K-nearest neighbors. Unsupervised methods (PCA and K-means) further reveal multivariate structure and clustering patterns among utilities. Findings show that while raw and standardized logistic models suffer from severe separation and unstable coefficient estimates, ridge logistic regression and tree-based models provide the most reliable predictive performance. When applied to 2023 data, the ridge model identifies Virginia as having a relatively high concentration of load-intensive utilities once weighted by retail sales and commercial activity, aligning with observed data center growth.\nThe study demonstrates how statistically rigorous, machine-learning-based classification can quantify evolving electricity demand patterns and provide state-level comparisons relevant to infrastructure planning. Limitations include incomplete utility-level metadata, non-uniform reporting across states, and the inability to observe direct data center loads. Nevertheless, the results provide a replicable framework for evaluating future changes in grid demand as data center development continues."
  },
  {
    "objectID": "projects/power.html#introduction-background",
    "href": "projects/power.html#introduction-background",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "2 Introduction & Background",
    "text": "2 Introduction & Background\n\n2.1 Motivation\nOver the past few decades, technological output has risen steadily. That gradual climb transformed into exponential growth with the advent of cloud computing, and now with the acceleration of the AI era that trajectory is poised to surge even faster. This rapid expansion is widely recognized. What’s far less understood, however, is the critical infrastructure that makes all of it possible. At the center of this digital revolution lies the often overlooked backbone of modern society: the power grid. It is the heartbeat of every computation, the lifeblood behind the flow of ones and zeros. Without this essential utility the modern world as we know it would come to a halt. And no where is that more true than in the state of Virginia.\n\n\n2.2 Virginia’s Internet & Data Boom\nOver the past several decades, Virginia has evolved from a strategically located government corridor into the epicenter of the modern internet. This transformation began in the early 1990s, when the Metropolitan Area Exchange–East (MAE-East)—one of the original four Network Access Points funded by the National Science Foundation—was established in Northern Virginia. Its proximity to federal agencies and research institutions such as DARPA, whose ARPANET project laid the foundations for the internet, positioned the region as a natural hub for digital networks. Over time, transatlantic subsea cable systems like MAREA-BRUSA, landing in Virginia Beach, further strengthened the state’s role in global connectivity [1].\nToday, that historical advantage has scaled into an unmatched concentration of data centers, making Virginia the digital heart of the world. An estimated 70% of global internet IP traffic is created in or routed through Loudoun County’s famed “Data Center Alley,” cementing Ashburn as the world’s central point for cloud and interconnection [2]. Virginia now hosts the largest cluster of data centers anywhere on Earth—surpassing Texas, California, and Illinois—with rapid expansion fueled by skyrocketing demand for cloud computing, AI workloads, and global digital services [3]. This boom is supported in part by Dominion Energy’s historically competitive electricity prices, which helped attract and retain power-intensive facilities. But it also places unprecedented strain on the state’s power grid: the very infrastructure that sustains the flow of computation. As AI and cloud demands continue to accelerate, Virginia’s electricity consumption is set to become one of the most consequential factors shaping both its economic future and the stability of its energy systems [1].\n\n\n2.3 Virginia’s Power Demands & Hyperscaler’s Damand for Data\nVirginia’s data center ecosystem has expanded at an unprecedented pace, driven largely by the accelerating demand for cloud computing and AI workloads. With more than 24,000 megawatts of planned data center capacity, Virginia surpasses every other market in the world—by more than a factor of three—in total expected power demand [3]. This explosive growth reflects the state’s long-standing role as the heart of global internet traffic, as well as its strategic appeal to the companies building the modern digital economy. Nowhere is this more evident than in Northern Virginia, where annual data center absorption has shattered global records; the region reached 270 MW of bookings in 2018, while no other market worldwide has ever exceeded 70 MW in a single year [2].\nAt the center of this surge are the hyperscalers1 major cloud and platform providers such as Microsoft, Google, Meta (Facebook), Apple, Oracle, IBM, Yahoo!, LinkedIn, Alibaba, Uber, and Dropbox. These companies require immense and rapidly scalable infrastructure, with hyperscale leases often ranging from 10 to 20 megawatts or more for a single deployment. The resulting demand for data is fueled by the relentless expansion of e-commerce, wireless networks, social media, streaming content, SaaS platforms, artificial intelligence, machine learning, gaming, virtual reality, and the Internet of Things. Collectively, these forces make Virginia not just a data center leader, but the world’s central engine for digital power consumption [1].\n\n\n2.4 The Fragile Power Grid\nThe power grid is essentially a giant, interconnected system that moves electricity from where it’s generated to where it’s needed. Power plants feed electricity into long-distance high-voltage transmission lines, which deliver power to substations where transformers reduce the voltage for safe distribution to homes, businesses, and—more recently—massive data centers. Because electricity cannot be stored easily at grid scale, supply and demand must remain in perfect balance at every moment. This requires constant coordination among utilities, transmission operators, and regional balancing authorities to maintain stability. When demand surges faster than supply, the grid becomes stressed, increasing the risk of brownouts or outages [4].\nThis balancing act becomes especially challenging in states like Virginia, where hyperscale data centers consume tens of megawatts each—orders of magnitude more than typical commercial users. As Virginia has become home to the world’s largest concentration of data centers, the strain on its power system has expanded accordingly. Each new facility adds continuous, around-the-clock demand that the grid must support without interruption. This is why long-term planning for new generation, transmission upgrades, and regional reliability is becoming central to Virginia’s energy future: the digital infrastructure powering cloud computing, AI, and global internet traffic can only grow if the physical grid underneath it can keep up.\nTo understand how these pressures play out across the country, it is helpful to situate Virginia within the broader structure of the United States energy landscape. The graphic below shows the U.S. Census Regions and Divisions, which group the nation into four major regions and nine divisions commonly used for national energy, economic, and demographic analysis.\n\n\n\n\n\n\nFigure 1: U.S. Census Regions and Divisions[^refnote].\n\n\n\n\n\n2.5 What Our Research Contributes\nTaken together, Virginia’s outsized role in global internet infrastructure, the explosive growth of hyperscale data centers, and the increasing fragility of the power grid form the backdrop for the analysis that follows. As demand for AI, cloud computing, and digital services accelerates, understanding how this demand translates into regional electricity usage becomes critical—not only for Virginia but for the nation. This study seeks to examine how Virginia’s utilities compare to those across the United States, identify whether data-center-driven growth produces distinctive load patterns, and explore the emerging pressures placed on the grid as a result. By combining national utility data with Virginia-specific trends, our goal is to measure where Virginia stands today, how its electricity profile is changing, and what these shifts may imply for energy planning, reliability, and the future of the digital economy."
  },
  {
    "objectID": "projects/power.html#data",
    "href": "projects/power.html#data",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "3 Data",
    "text": "3 Data\n\n3.1 Dataset Overview\nFor this analysis we used the Annual Electric Power Industry Report (Form EIA-861) which is a U.S. government survey (considered the Census for Utilities) conducted by the Energy Information Administration (EIA) that collects detailed data from electric utilities on retail sales, revenue, and other key aspects of the electric power industry. It provides information on energy sources, demand-side management, net-metering, and other operational data to help monitor the industry’s status and trends [5].\nOriginally we wanted to know about the impact of data centers in local communities in northern VA, had reached out to several city planners, they responded back but revealed that getting specific data on individual data centers would require coordination with individual commercial power supply companies and the individual data centers themselves which would be difficult or impossible to obtain given various NDAs that would need to be signed and potential limitations on what we could share. Alas, the annually released data that is generated compiled by the EIA contains the main companies we are interested in at least their bulk aggregated numbers which will be discussed later. For example, doing a little research on line revealed that the utility named Virginia Electric & Power Co whcih is found in the data is really part of Dominion Energy which is one of the biggest power utilities in Virginia. The dataset also allows for studying behavioral indicators identifying utilities that exhibit data center like pattern such as:\n\nHigh commercial load\nVery high commercial electricity sales\nLarge sales per customer\nHigh commercial sales per commercial customer\n\nThese variables become proxies for “data-center–like” behavior.\nA data dictionary2 of the various data elements used in this analysis can also be referenced.\n\n\n3.2 Limitations\nThe dataset does have some obvious limitations. For one, we are looking at a yearly aggregation of data which provides a very course view of the data. This limitation does now allow us to view individual data centers or even the break down of the individual contributors of what make up a utility. There might be some missing data points as well. Some private commercial companies might not have released certain data in the survey which might leave gaps in the analysis and greatly impact the value of our model’s predictions. Despite these limitations, the EIA-861 data has been used in several analysis reports and can be viewed as a reliable data source for understanding macro trends seen in the United States power grid. Later on in the analysis section we will construct various methods of overcoming these limitations however crude they might be.\n\n\n3.3 Data Cleaning, Processing & Assumptions\nWe initially started our analysis on the 2017 EIA-861 curated dataset developed by the CORGIS Dataset Project [6]. However, when comparing the numbers for 2017 to the original data source the numbers were off. This might have been due to updates to upstream data that never reached the curated dataset. After this discovery we created a python script to regenerate the same exact dataset (same fields as the CORGIS Dataset) not only for 2017 but for 10 years worth of data. So 2015 to 2015. This script basically parsed each of the zipped files from source and extracted data from 3 different schedules:\n\nOperational Data - The data containing aggregate operational data for the source and disposition of energy and revenue information from each electric utility.\nUtility Data - The data contain information on a utility’s North American Electric Reliability (NERC) regions of operation. The data also indicate a utility’s independent system operator (ISO) or regional transmission organization (RTO) and whether that utility is engaged in any of the following activities: generation, transmission, buying transmission, distribution, buying distribution, wholesale marketing, retail marketing, bundled service, or operating alternative-fueled vehicles.\nSales to Ultimate Customers - The data contain revenue, sales (in megawatthours), and customer count of electricity delivered to end-use customers by state, sector, and balancing authority. A state, service type, and balancing authority-level adjustment is made for non-respondents and for customer-sited respondents.\n\nWhen it comes to data cleaning we had to handle the following and make some assumptions:\n\nIn EIA-861 spreadsheets a dot (.) could represent data that was suppressed (as mentioned before with private companies), not available, or did not meet a certain threshold value. For these values we decided to make zero but might use soem form of data imputation to fill these in.\nWe found only a handful of empty values in the state field. We dropped these rows as knowing where the utility is located was critical to the analysis.\n\n\n\n3.4 Derived Featurs - Feature Engineering\nWe define the following engineered variables used throughout the analysis. Much of these variables are based on potentially biased opinions of what we might expect a data center like conditons to look like.\n\n3.4.1 Commercial Ratio\nMeasures how much of a utility’s total load comes from the commercial sector. High ratios suggest large commercial customers such as data centers. \\[\n\\text{Commercial\\_Ratio}\n\\;=\\;\n\\frac{\\text{Commercial Sales}}{\\text{Total Sales}}\n\\]\n\n\n3.4.2 Sales per Customer\nCaptures average consumption intensity across all customers. We would expect data centers to produce extremely high sales relative to customer count, making this a strong signal. \\[\n\\text{SalesPerCustomer}\n\\;=\\;\n\\frac{\\text{Total Sales}}{\\text{Total Customers}}\n\\]\n\n\n3.4.3 Commercial Sales per Commercial Customer (SPC)\nMeasures electricity use per commercial customer. Data centers typically appear as a small number of commercial customers with very high consumption, pushing this metric upward. \\[\n\\text{Commercial\\_SPC}\n\\;=\\;\n\\frac{\\text{Commercial Sales}}{\\text{Commercial Customers}}\n\\]\n\n\n3.4.4 Losses Ratio\nRepresents distribution and transmission losses relative to total usage. Extremely high-volume industrial or data center load can shift loss3 patterns on the grid.[7] \\[\n\\text{Losses\\_Ratio}\n\\;=\\;\n\\frac{\\text{Losses}}{\\text{Total Uses}}\n\\]\n\n\n3.4.5 Log-Transformed Size Features\nLog transforms stabilize variance and capture utility scale without being dominated by extreme outliers. During our EDA we discovered some issues with the data. Total customers and total sales vary by several orders of magnitude across U.S. utilities. Some are tiny rural co-ops; others (like Dominion) serve millions. The log transformation helps to mitigate these skewness issues and improves stablity in our modeling. In this study, logging these variables prevents large utilities from overwhelming the models and provides more interpretable patterns across the full national dataset. \\[\n\\log(\\text{Total\\_Customers}),\n\\qquad\n\\log(\\text{Total\\_Sales})\n\\]\n\n\n\n3.4.6 Composite Intensity Score\nCombines three load-intensity metrics (Commercial Ratio, SPC, SalesPerCustomer) into a single standardized indicator of “data-center–like” consumption profiles.\nEach variable above is standardized to a z-score. The composite Intensity Score is then defined as:\n\\[\n\\text{IntensityScore}\n\\;=\\;\nz(\\text{Commercial\\_Ratio})\n\\;+\\;\nz(\\text{SalesPerCustomer})\n\\;+\\;\nz(\\text{Commercial\\_SPC})\n\\]\n\n\n\n3.4.7 High-Demand Label\nIdentifies the top decile of utilities exhibiting exceptionally intense commercial electricity use those most likely associated with data center load characteristics.\nUtilities falling in the top 10% of the Intensity Score distribution are classified as high-demand:\n\\[\n\\text{HighDemand} = 1\n\\quad\\text{if}\\quad\n\\text{IntensityScore} \\ge Q_{0.90}(\\text{IntensityScore})\n\\]\nand otherwise:\n\\[\n\\text{HighDemand} = 0\n\\]\n\n\n\n3.5 Exploratory Data Analysis\nThe number of reporting utilities varies substantially by state.\nFigure Figure 2 shows the distribution of utilities across states in 2023, with Virginia highlighted for comparison. As you can see, Texas, California and New York are highly skewed and represent the vast majority of the number of utilities. In order to compare Virginia to the rest of the states we will need to handle this.\n\n\n\n\n\n\n\n\nFigure 2: Number of utilities per state in 2023, with Virginia highlighted.\n\n\n\n\n\nWe also examine the mix of utility business types (investor-owned, municipal, cooperative, etc.).\nFigure Figure 3 summarizes the count of utilities by type in 2023. For this particular study we are interested mostly in Investor Owned, Cooperative and Retail Power Marketers but we will consider all utilities.\n\n\n\n\n\n\n\n\nFigure 3: Number of utilities by type in 2023.\n\n\n\n\n\nFigure Figure 4 shows the skewed nature of the raw variables. Log-transforming the data (Figure Figure 5) improves interpretability.\n\n\n\n\n\n\n\n\nFigure 4: Distributions of key electricity variables in 2023 (original scale).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distributions of key electricity variables in 2023 (log10 scale).\n\n\n\n\n\nSector composition across utilities varies widely (Figure Figure 6). Virginia’s utilities show distinct distributional patterns compared with the rest of the U.S. (Figure Figure 7).\n\n\n\n\n\n\n\n\nFigure 6: Distribution of key electricity sector ratios across all U.S. utilities in 2023.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Comparison of sector ratios between Virginia utilities and all other U.S. utilities in 2023.\n\n\n\n\n\nFigure Figure 8 shows that Virginia utilities generally have higher per-customer intensity levels, especially for commercial customers.\n\n\n\n\n\n\n\n\nFigure 8: Comparison of per-customer electricity intensity measures for Virginia utilities versus all other U.S. utilities (2023). Values shown on a log scale.\n\n\n\n\n\nFigure Figure 9 shows how Virginia utilities compare to national utilities in overall system size and total electricity demand.\n\n\n\n\n\n\n\n\nFigure 9: Relationship between total customers and total electricity sales (log–log scale) for U.S. utilities in 2023, highlighting Virginia utilities.\n\n\n\n\n\nFigure Figure 10 highlights the commercial load structure and intensity for Virginia relative to other states.\n\n\n\n\n\n\n\n\nFigure 10: Commercial sector structure versus commercial sales per customer (log scale) for U.S. utilities in 2023, with Virginia utilities highlighted.\n\n\n\n\n\nFigure Figure 11 shows how Virginia’s average electricity intensity per customer compares to the national trend from 2015–2024.\n\n\n\n\n\n\n\n\nFigure 11: Average electricity sales per customer over time (2015–2024), comparing Virginia to all other U.S. states.\n\n\n\n\n\nFigure Figure 12 illustrates how Virginia’s commercial load share differs from the rest of the U.S. over time.\n\n\n\n\n\n\n\n\nFigure 12: Average commercial electricity sales ratio over time (2015–2024), comparing Virginia to all other U.S. states."
  },
  {
    "objectID": "projects/power.html#research-questions",
    "href": "projects/power.html#research-questions",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "4 Research Questions",
    "text": "4 Research Questions\nFor this analysis we wanted to be able to answer the following questions:\n\nCan we classify which utilities exhibit “data-center–like” load characteristics based on commercial intensity and per-customer electricity usage?\nHow does Virginia compare nationally in the proportion and concentration of high-demand utilities?\nWhich states show strong dominance of high-load utilities when weighted by retail sales, customer base, and commercial sales volume?\n\n\n4.1 Building Classifiers\nIn this section we explore methods that will enable us to classify which utilities exhibit “data-center–like” load characteristics based on commercial intensity and per-customer electricity usage. In particular we look at various models including logistic regression, KNN, trees and random forests that will later enable us to not only identify utilities for the year we trained on but also generalize to other years. And lastly we look at some clustering methods using PCA and K-means.\nFor all models we split our data into training and test sets using a 20% hold out for testing. We set a random seed of 123 which should allow us to have consistent results. We also use our featurize method to do feature engineering yielding out high demand characteristic features mentioned in the Data section.\nFigure Figure 13 shows the distribution of high demand utilities across the country for 2023. This is after we do feature engineering on our data set. As you can see there are a very small number of high demand utilities.\n\n\n\n\n\n\n\n\nFigure 13: Distribution of high demand utilities for (2023)\n\n\n\n\n\nFigure Figure 14 illustrates how each of our feature engineered predictor variables of Commercial_Ratio, Commercial_SPC, Losses_Ratio, and SalesPerCustomer are highly skewed without any data transformations.\n\n\n\n\n\n\n\n\nFigure 14: Distributions of Predictors (original scale)\n\n\n\n\n\nFigure Figure 15 now shows that with a log10 transformation, our feature engineered predictor variables all exhibit much more clearer and interpretable distributions.\n\n\n\n\n\n\n\n\nFigure 15: Distributions of Predictors (log10 scale)\n\n\n\n\n\nFigure Figure 16 displays boxplots of our 5 predictors, separated by demand class. Note that while observing these boxplots, we can see that the high demand class is often correlated with much higher average Commercial_Ratio values, as well as lower average Losses_Ratio and log_Total_Customers values.\n\n\n\n\n\n\n\n\nFigure 16: Boxplots of Predictors by High-Demand Class (2023)\n\n\n\n\n\n\n4.1.1 Logistic Regression\nWe first look at the unpenalized logistic regression model. This model attempted to estimate how well our utility characteristics predict whether a utility falls into the high demand or high intensity distribution (HighDemand = 1). Although the model achieved high classification accuracy, the coefficient estimates were extremely large, indicating numerical instability. This occurs when predictors nearly perfectly separate HighDemand from non-HighDemand utilities. In such cases, the logistic maximum likelihood estimator pushes coefficients toward infinity, producing inflated parameter estimates and artificially tiny p-values. This makes interpreting Model Equation 1 difficult.\nDue to this instability, the individual coefficients cannot be interpreted in terms of effect sizes, odds ratios, or directionality. The model is useful only as a classifier but not as an explanatory model. The extreme magnitudes also reflect high multicollinearity among engineered intensity variables, differences in scale, and the presence of many dummy variables for utility type categories. These limitations motivate the use of penalized logistic regression or simpler composite scoring methods for more stable and interpretable results which we will explore later.\nSee the results of this model in Table 1.\nAs shown in Model Equation 1, the probability of a utility being classified as high-demand depends on several structural and commercial intensity features.\n\\[\n\\begin{aligned}\n\\Pr(Y_i = 1 \\mid X_i)\n&= \\frac{1}{1 + \\exp(-\\eta_i)} \\\\\n\\\\\n\\eta_i =\\;\n&\\beta_0\n+ \\beta_1\\,\\text{Commercial Ratio}_i \\\\\n&+ \\beta_2\\,\\text{Sales per Customer}_i \\\\\n&+ \\beta_3\\,\\text{Commercial SPC}_i \\\\\n&+ \\beta_4\\,\\text{Losses Ratio}_i \\\\\n&+ \\beta_5\\,\\log(\\text{Total Customers}_i) \\\\\n&+ \\beta_6\\,\\text{is VA}_i \\\\\n&+ \\sum_k \\gamma_k\\,\\text{Utility Type}_{ik}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\n\nTable 1: Confusion Matrix for Unpenalized Logistic Regression Model (Test Set)\n\n\n\n\nConfusion Matrix\n\n\n\n0\n1\n\n\n\n\n0\n274\n6\n\n\n1\n1\n33\n\n\n\n\n\n\n\n\nClassification.Accuracy\n\n\n\n\n0.9777\n\n\n\n\n\n\n\n\nThe standardized logistic regression model Equation 2 replaces the raw intensity variables with their z-scores while keeping the same outcome (HighDemand_intensity) and covariates (is_VA and Utility.Type). Standardization improves numerical conditioning and makes predictors comparable in principle, but in this case the fitted model still shows extremely large coefficient estimates and tiny p-values for all terms. This indicates that the underlying problem has not been resolved.\n\\[\n\\Pr(Y_i = 1 \\mid X_i)\n= \\frac{1}{1 + \\exp(-\\eta_i)}\n\\]\n\\[\n\\begin{aligned}\n\\eta_i =\\;\n&\\beta_0\n+ \\beta_1\\,z\\!\\left(\\text{Commercial Ratio}_i\\right)\n+ \\beta_2\\,z\\!\\left(\\text{Sales per Customer}_i\\right) \\\\\n&+ \\beta_3\\,z\\!\\left(\\text{Commercial SPC}_i\\right)\n+ \\beta_4\\,z\\!\\left(\\text{Losses Ratio}_i\\right)\n+ \\beta_5\\,z\\!\\left(\\log(\\text{Total Customers}_i)\\right) \\\\\n&+ \\beta_6\\,\\text{is VA}_i\n+ \\sum_k \\gamma_k\\,\\text{Utility Type}_{ik}\n\\end{aligned}\n\\tag{2}\\]\nAs with the unstandardized model, this standardized specification achieves high classification accuracy on the test set (about 97.8% see Table 2), but the individual coefficients cannot be interpreted as meaningful effect sizes or odds ratios. The model is functioning as an overfit classifier rather than a stable explanatory model. In practice, this motivates relying on penalized models, simpler composite indices, and descriptive comparisons.\n\n\n\n\nTable 2: Confusion Matrix for Standardized Logistic Regression Model (Test Set)\n\n\n\n\nConfusion Matrix (Standardized Logistic Model)\n\n\n\n0\n1\n\n\n\n\n0\n274\n6\n\n\n1\n1\n33\n\n\n\n\n\n\n\n\nClassification.Accuracy\n\n\n\n\n0.9777\n\n\n\n\n\n\n\n\nTo address the instability observed in the unpenalized logistic regression models, we fit a Ridge-penalized logistic regression using cross-validation. Ridge regression introduces an L2 penalty on the magnitude of the coefficients, shrinking them toward zero and preventing the divergence that occurred in the raw and standardized models. We trained the model using 10-fold cross-validation, which automatically selects the value of the penalty parameter \\(\\lambda\\) that minimizes cross-validated deviance.\nThe Ridge model produces well-behaved coefficients and yields more reliable out-of-sample predictions. Unlike the previous models, Ridge does not aim to provide easily interpretable coefficients penalization introduces bias in exchange for substantial variance reduction but it provides a more stable and generalizable classifier. See Table 3.\nClassification performance on the test set remains very strong, with an accuracy near 97–98%, similar to the unpenalized models. However, because the coefficients are shrunk and closely correlated predictors share weight, Ridge should be interpreted as a predictive model, not an explanatory one.\n\n\n\n\nTable 3: Confusion Matrix for Ridge Logistic Regression Model (Test Set)\n\n\n\n\nConfusion Matrix (Ridge Logistic Model)\n\n\n\n0\n1\n\n\n\n\n0\n275\n17\n\n\n1\n0\n22\n\n\n\n\n\n\n\n\nClassification.Accuracy\n\n\n\n\n0.9459\n\n\n\n\n\n\n\n\nThe Table 4 shows a comparison of the 3 different logistic classifier models.\n\n\n\n\nTable 4: Comparison of Logistic Regression Models for High-Demand Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nTraining.Issues\nInterpretability\nAIC\nTest.Accuracy\nKey.Notes\n\n\n\n\nRaw Logistic Regression\nSevere separation; coefficients explode\nPoor (coefficients meaningless)\n604.7\n0.9777\nNot reliable for inference; unstable estimates\n\n\nStandardized Logistic Regression\nStill unstable; separation persists\nPoor (scaled version of same issue)\n604.7\n0.9777\nStandardization alone did not fix the problem\n\n\nRidge Logistic Regression\nStable; L2 penalty prevents divergence\nModerate (penalized, but stable)\nNA\n0.9459\nBest predictive model; generalizes well\n\n\n\n\n\n\n\n\n\n\n4.1.2 Tree Classifier\n\n\nn= 1253 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 1253 118 0 (0.905826018 0.094173982)  \n  2) Commercial_Ratio&lt; 0.7096691 1144   9 0 (0.992132867 0.007867133)  \n    4) Commercial_SPC&lt; 12663.81 1135   0 0 (1.000000000 0.000000000) *\n    5) Commercial_SPC&gt;=12663.81 9   0 1 (0.000000000 1.000000000) *\n  3) Commercial_Ratio&gt;=0.7096691 109   0 1 (0.000000000 1.000000000) *\n\n\n         Actual\nPredicted   0   1\n        0 273   1\n        1   2  38\n\n\n[1] 0.9904459\n\n\n\n\nTable 5: Confusion Matrix for Tree Model (Test Set)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.3 Random Forest\n\n\n\nCall:\n randomForest(formula = factor(HighDemand_intensity) ~ Commercial_Ratio +      SalesPerCustomer + Commercial_SPC + Losses_Ratio + log_Total_Customers +      is_VA + Utility.Type, data = train, ntree = 500, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0.4%\nConfusion matrix:\n     0   1 class.error\n0 1132   3 0.002643172\n1    2 116 0.016949153\n\n\n         Actual\nPredicted   0   1\n        0 273   2\n        1   2  37\n\n\n[1] 0.9872611\n\n\n\n\nTable 6: Confusion Matrix for Random Forest Model (Test Set)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.4 KNN\nK-Nearest Neighbors (KNN) analysis begins by selecting predictors and standardizing them using the mean and standard deviation calculated from the training data. This ensures that all features contribute equally to the distance calculations used by KNN. The standardized training data and corresponding class labels (HighDemand_intensity) are then used to classify each observation in the standardized test set by identifying the k=5 closest training points in feature space, and assigning the majority class among those neighbors. Finally, predictions are compared with actual test labels to generate a confusion matrix and accuracy measure, as shown in Table 7.\nThe results show that the model correctly classified nearly all observations, incorrectly classifying only 10 out of 314 cases and achieving an accuracy of about 96.8%. The confusion matrix also indicates no false positives, suggesting the classifier is conservative and highly accurate for the High Demand class.\n\n\n\nTable 7: KNN\n\n\n\n         Actual\nPredicted   0   1\n        0 275  10\n        1   0  29\n\n\n[1] 0.9681529\n\n\n\n\n\n\n4.1.5 PCA and K-means Unsupervised Approach\nPrincipal Component Analysis (PCA) was used to identify the dominant structural patterns in the engineered utility features. This approach is non-parametric and unsupervised so cannot be compared to other models explored. However, it can give us more insight into various patterns within the data.\nThe first two principal components explained a large share of total variance, indicating that most variation in utility intensity can be summarized in two latent dimensions. See the PCA Scree Plot Table 8.\nJust for reference:\nPC1 = Commercial intensity axis i.e. Commercial Ratio & Log(Total Customers)\nPC2 = Customer scale axis i.e. SalesPerCustomer & Commercial SPC\n\n\n\n\nTable 8: PCA Scree Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the PCA scatter plot Table 9, high‐demand utilities tend to occupy distinct regions of PC space, demonstrating that the engineered ratios and per-customer metrics capture meaningful behavioral differences. The plot shows High-demand utilities are far right on PC1. Virginia utilities are spread out and not concentrated in the high-intensity clusters.\n\n\n\n\nTable 9: PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-means clustering (see Table 10) applied to the first three PCs further revealed that utilities naturally group into clusters corresponding to intensity characteristics. One cluster contained a disproportionate share of high-demand utilities, confirming that their behavior is structurally different from typical utilities. When examining Virginia utilities, several appear near the edges of clusters, suggesting that Virginia’s utilities exhibit more extreme or atypical load patterns relative to national norms. PCA + K-means shows Virginia does NOT cluster with high-intensity utilities. Only Dominion and NoVA co-ops show mild PC1 shifting — but not enough to enter the high-intensity group.\n\n\n\n\nTable 10: PCA and K-means Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, PCA reveals that Virginia utilities do not form a distinct high-intensity cluster. Instead, they occupy mid-range positions in the commercial-load space, consistent with a mixed customer base rather than pure commercial concentration.\n\n\n4.1.6 Model Comparison Summary\nBoth ridge logistic and random forest models reliably identify utilities with unusually intense commercial electricity usage. See Table 11.\n\n\n\n\nTable 11: Model Summary\n\n\n\n\nModel Performance Comparison\n\n\nModel\nAccuracy\nNotes\n\n\n\n\nLogistic Regression (Raw)\n0.9777\nCoefficient explosion; separation issues\n\n\nLogistic Regression (Scaled)\n0.9777\nStill unstable but standardized\n\n\nRidge Logistic Regression\n0.9459\nMost stable logistic model; good generalization\n\n\nClassification Tree\n0.9904\nLow-variance, simple interpretable model\n\n\nRandom Forest\n0.9873\nHighest-performing tree-based model\n\n\nKNN (k = 5)\n0.9682\nSensitive to scaling; depends on local structure\n\n\n\n\n\n\n\n\n\n\n\n4.2 Predictive Modeling of High-Load Utility Distribution and State-Level Dominance\nNow that we have built several models we want to utilize those models to answer our remaining research questions:\n\nHow does Virginia compare nationally in the proportion and concentration of high-demand utilities?\nWhich states show strong dominance of high-load utilities when weighted by retail sales, customer base, and commercial sales volume?\n\n\n4.2.1 Methodology\nTo answer these questions, we first considered our trained ridge-penalized logistic regression model trained on 2023 utility-level data to classify each utility as either high-demand or normal-demand based on our engineered intensity features. We then applied this fitted model, along with a random forest classifier, to out-of-sample years (2018 and 2023) by re-featurizing the EIA-861 data for each year and carefully aligning factor levels and matrix columns with the original training set. This produced a combined dataset (classified_all) containing predicted high-demand labels for every utility, year, and state. From this, we constructed state-level summaries:\n\nthe proportion of utilities classified as high-demand (to measure concentration), and\na composite Data Center Load Index (DCLI) Equation 3 that averages the share of total retail sales, customers, and commercial sales served by high-demand utilities (to measure dominance). The resulting state-level proportions and DCLI values were then visualized to compare Virginia’s position to that of other states.\n\n\\[\n\\text{DCLI}_s\n=\n\\frac{\n\\left(\n\\frac{\\text{HighSales}_s}{\\text{TotalSales}_s}\n\\right)\n+\n\\left(\n\\frac{\\text{HighCustomers}_s}{\\text{TotalCustomers}_s}\n\\right)\n+\n\\left(\n\\frac{\\text{HighCommercialSales}_s}{\\text{TotalCommercialSales}_s}\n\\right)\n}{3}\n\\tag{3}\\]\nWhere:\n\n\\(\\mathcal{U}_s\\) = set of all utilities in state \\(s\\)\n\n\\(\\mathcal{H}_s \\subset \\mathcal{U}_s\\) = subset of high-demand utilities in state \\(s\\)\n\nSales = total annual retail electricity sales\n\nCustomers = total annual number of customers\n\nCommSales = total annual commercial-sector electricity sales\n\n\n\n4.2.2 Results\nAs can be seen in Figure 17, across all states, we find that Virginia does not emerge as the single most extreme state in terms of the number of utilities classified as high-demand. Instead, it ranks roughly tenth, placing it in the top ten nationally. This means that Virginia’s utility mix is unmistakably more “high-intensity” than that of the typical U.S. state, but several states exhibit even higher concentrations of load-intensive utilities. These findings indicate that while Virginia is a key center of high-demand electricity usage, it is not unique in having utilities that show strong commercial and per-customer intensity profiles.\n\n\n\n\n\n\n\n\nFigure 17: Proportion of high-demand utilities (Virginia vs national) - Ridge Model\n\n\n\n\n\nHowever, simple utility counts do not reflect overall dominance, because not all utilities are equal in size or influence. To address this, we constructed the Data Center Load Index (DCLI) a composite metric averaging the retail-sales-weighted, customer-weighted, and commercial-sales-weighted share of total load served by high-demand utilities. This index captures not just how many high-demand utilities a state has, but how important they are to the state’s power demand. As can be seen in Figure 18 under the DCLI, Virginia again appears in the upper tier of states, but is outpaced by a handful of states where one or two utilities dominate nearly the entire statewide load. Importantly, several of these top-ranked states are not traditional data center hubs, suggesting that our intensity-based classification is detecting a broader category of industrial and commercial high-load behavior not only hyperscale data centers.\n\n\n\n\n\n\n\n\nFigure 18: High-demand utility concentration and weighted dominance across states, highlighting Virginia.\n\n\n\n\n\n\n\n4.2.3 Discussion & Limitations\nThe DCLI reveals why Virginia ranks highly even though it has relatively few high-demand utilities. Virginia’s electricity system is anchored by a small number of extremely large utilities, most notably Dominion Energy and NOVEC. If one of these utilities is classified as high-demand, it automatically commands a very large share of total statewide sales and customers. In other words:\nVirginia is not dominated by many high-demand utilities—it is dominated by a few enormous ones.\nThis explains Virginia’s strong DCLI ranking: high-demand load is concentrated inside a small number of massive utilities, rather than spread across many smaller ones as occurs in other states. By contrast, states like New York exhibit a high count of high-demand utilities, but these tend to be relatively small; their DCLI drops sharply once weighted by load. Other states appear at the top of the rankings due to industrial intensity where load patterns resemble the characteristics of our “high-demand” classification even though they are not data center hubs.\nThese findings highlight both the strength and the limits of using utility-level EIA-861 data as a proxy for identifying data-center-like demand. Because the dataset does not identify data centers explicitly, the “high-demand” class inevitably captures a mixture of true hyperscale activity and unrelated high-load commercial or industrial customers. Rather than contradicting Virginia’s well-documented role as the world’s largest data center market, the results show that when we zoom out to a national, utility-level lens, data-center-like load is part of a broader pattern of concentrated high-intensity commercial usage across the United States.\nBringing everything together, the proportional rankings and DCLI analysis provide a consistent story: Virginia stands out as one of the nation’s most high-demand states not because it has many high-demand utilities, but because the few that it does have carry enormous weight. When considering both concentration and dominance, Virginia emerges clearly as a high-intensity electricity state whose load structure reflects the presence of major hyperscale data center activity layered onto large, diversified utility territories."
  },
  {
    "objectID": "projects/power.html#question-1",
    "href": "projects/power.html#question-1",
    "title": "Modeling and Predicting Virginia’s Electricity Usage in the Era of Data Centers: A Statistical Learning Comparison with National Utility Trends",
    "section": "Question 1",
    "text": "Question 1\n\nMethods\n\n\nResults"
  },
  {
    "objectID": "projects/power.html#question-2",
    "href": "projects/power.html#question-2",
    "title": "Modeling and Predicting Virginia’s Electricity Usage in the Era of Data Centers: A Statistical Learning Comparison with National Utility Trends",
    "section": "Question 2",
    "text": "Question 2\n\nMethods\n\n\nResults"
  },
  {
    "objectID": "projects/power.html#question-3",
    "href": "projects/power.html#question-3",
    "title": "Modeling and Predicting Virginia’s Electricity Usage in the Era of Data Centers: A Statistical Learning Comparison with National Utility Trends",
    "section": "Question 3",
    "text": "Question 3\n\nMethods\n\n\nResults"
  },
  {
    "objectID": "projects/power.html#abstract",
    "href": "projects/power.html#abstract",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "",
    "text": "The rapid expansion of data centers in Virginia has raised concerns about rising electricity consumption and shifting load patterns especially in northern counties of the state. This study leverages a statistical learning approach to identify high-demand electric utilities and to compare Virginia’s demand characteristics with national trends. Using ten years of U.S. Energy Information Administration (EIA-861) data, we construct engineered features such as commercial sales ratios, per-customer intensity metrics, and loss ratios—and compute a composite Intensity Score to classify utilities exhibiting unusually high load profiles.\nMultiple supervised models are evaluated, including logistic regression, standardized logistic regression, ridge-penalized logistic regression, classification trees, random forests, and K-nearest neighbors. Unsupervised methods (PCA and K-means) further reveal multivariate structure and clustering patterns among utilities. Findings show that while raw and standardized logistic models suffer from severe separation and unstable coefficient estimates, ridge logistic regression and tree-based models provide the most reliable predictive performance. When applied to 2023 data, the ridge model identifies Virginia as having a relatively high concentration of load-intensive utilities once weighted by retail sales and commercial activity, aligning with observed data center growth.\nThe study demonstrates how statistically rigorous, machine-learning-based classification can quantify evolving electricity demand patterns and provide state-level comparisons relevant to infrastructure planning. Limitations include incomplete utility-level metadata, non-uniform reporting across states, and the inability to observe direct data center loads. Nevertheless, the results provide a replicable framework for evaluating future changes in grid demand as data center development continues."
  },
  {
    "objectID": "projects/power.html#conclusion",
    "href": "projects/power.html#conclusion",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis study demonstrates that utility-level EIA-861 data, despite lacking explicit data-center identifiers, can still reveal meaningful data-center–like electricity demand patterns when combined with structural load metrics, machine-learning classifiers, and weighted dominance indices such as the DCLI. Our results show that Virginia consistently ranks within the upper tier of states in both the proportion and dominance of high-demand utilities, though it is not the single most extreme case nationally. Instead, Virginia’s position is driven by a small number of very large utilities whose load profiles heavily influence statewide metrics—highlighting that intensity and dominance do not always correspond to the number of high-demand utilities alone.\nKey insights gained from this study include:\n\nHigh-demand utility behavior is not unique to traditional data-center states; several states exhibit strong commercial or industrial load intensity unrelated to hyperscale development.\nVirginia’s system stands out not because it has many high-demand utilities, but because its few high-intensity utilities represent a disproportionately large share of statewide load.\nWeighted metrics such as the DCLI provide a more accurate picture of systemic dominance than simple counts of high-demand utilities.\n\nFuture considerations:\n\nMore granular data such as substation-level load, hourly consumption, or direct data-center reporting would improve classification accuracy and help isolate genuine data-center demand from other high-load sectors.\nIncorporating temporal dynamics through time-series models could reveal how high-demand behavior evolves as data-center growth accelerates.\nAdditional feature engineering or alternative classification approaches may help reduce noise in states where industrial load patterns resemble data-center behavior.\n\nOverall, this framework provides a scalable foundation for tracking evolving electricity demand pressures and assessing how data-center–like load patterns shape the resilience and planning needs of state-level power systems."
  },
  {
    "objectID": "projects/power.html#references",
    "href": "projects/power.html#references",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "6 References",
    "text": "6 References"
  },
  {
    "objectID": "projects/power.html#related-works",
    "href": "projects/power.html#related-works",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "3 Related Works",
    "text": "3 Related Works\nCongress\nCrypto\nDemand\nCloud Next Door\nAs shown in recent studies texas-power-demand?, data centers significantly increase load.\nTODO: Discuss 3 (already selected) other studies and approaches that have been used to understand these issues. How does our reserach differ and what new contributions do we have to add?"
  },
  {
    "objectID": "projects/power.html#footnotes",
    "href": "projects/power.html#footnotes",
    "title": "Identifying High-Demand Utilities in Virginia: Statistical Learning and National Comparative Analysis in the Era of Data Centers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA hyperscaler is a company that operates massive-scale data centers to provide large-scale cloud computing services, like computing, storage, and networking, that can be rapidly scaled to meet fluctuating demand.↩︎\nhttps://corgis-edu.github.io/corgis/csv/electricity/↩︎\nTransmission and distribution losses represent the electricity that is generated but never reaches customers. This energy is lost as heat in power lines and transformers as it travels across the grid.↩︎"
  },
  {
    "objectID": "projects/power.html#composite-intensity-score",
    "href": "projects/power.html#composite-intensity-score",
    "title": "Modeling and Predicting Virginia’s Electricity Usage in the Era of Data Centers: A Statistical Learning Comparison with National Utility Trends",
    "section": "Composite Intensity Score",
    "text": "Composite Intensity Score\nEach variable above is standardized to a z-score.\nThe composite Intensity Score is then defined as:\n\\[\n\\text{IntensityScore}\n\\;=\\;\nz(\\text{Commercial\\_Ratio})\n\\;+\\;\nz(\\text{SalesPerCustomer})\n\\;+\\;\nz(\\text{Commercial\\_SPC})\n\\]"
  },
  {
    "objectID": "projects/power.html#high-demand-label",
    "href": "projects/power.html#high-demand-label",
    "title": "Modeling and Predicting Virginia’s Electricity Usage in the Era of Data Centers: A Statistical Learning Comparison with National Utility Trends",
    "section": "High-Demand Label",
    "text": "High-Demand Label\nUtilities falling in the top 10% of the Intensity Score distribution are classified as high-demand:\n\\[\n\\text{HighDemand} = 1\n\\quad\\text{if}\\quad\n\\text{IntensityScore} \\ge Q_{0.90}(\\text{IntensityScore})\n\\]\nand otherwise:\n\\[\n\\text{HighDemand} = 0\n\\]\n\nExploratory Data Analysis"
  }
]